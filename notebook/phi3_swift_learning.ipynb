{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Training Phi-3-mini-128k-instruct to Learn Swift Programming Language\n\nThis notebook trains Microsoft's Phi-3-mini-128k-instruct model to understand and work with Swift code using a dataset of real Swift files.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Install required libraries\n!pip install transformers datasets evaluate torch scikit-learn tqdm dropbox requests accelerate peft bitsandbytes",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Important: These imports must be properly separated\nimport os\nimport json\nimport torch\nimport random\nimport numpy as np\nimport time\nimport gc\nimport re\nimport collections\nimport psutil  # Add psutil for memory monitoring\nfrom tqdm.auto import tqdm\nfrom datasets import load_dataset, ClassLabel\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM,\n    Trainer, \n    TrainingArguments,\n    set_seed,\n    DataCollatorForLanguageModeling,\n    EarlyStoppingCallback,\n    BitsAndBytesConfig\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n# Note: We're using string literals for task_type instead of importing TaskType enum to avoid compatibility issues\n\n# Set a seed for reproducibility\nset_seed(42)\n\n# Add memory management function with more detailed reporting\ndef cleanup_memory():\n    \"\"\"Force garbage collection and clear CUDA cache if available.\"\"\"\n    # Get memory usage before cleanup\n    before = psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2\n    \n    # Perform cleanup\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    # Get memory usage after cleanup\n    after = psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2\n    print(f\"Memory cleaned up. Before: {before:.2f} MB, After: {after:.2f} MB, Freed: {before - after:.2f} MB\")\n    \n    # Print system memory info\n    mem = psutil.virtual_memory()\n    print(f\"System memory: {mem.percent}% used, {mem.available / 1024 / 1024:.2f} MB available\")\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Check if GPU is available\nimport torch\n\nif torch.cuda.is_available():\n    device = torch.device('cuda')\n    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\nelse:\n    device = torch.device('cpu')\n    print(\"Using CPU - Note: Training will be much slower on CPU\")\n\n# Set random seeds for reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Dataset and Model Configuration\n\nLet's define the model and dataset we'll be using:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Dataset configuration - using the same dataset as the original notebook\nDATASET_ID = \"mvasiliniuc/iva-swift-codeint\"\n\n# Model configuration - using Phi-3-mini-128k-instruct\nMODEL_NAME = \"microsoft/Phi-3-mini-128k-instruct\"\nMAX_LENGTH = 4096  # Phi-3 can handle long sequences natively\nBATCH_SIZE = 4  # Reduced batch size due to model size\nLEARNING_RATE = 2e-5\nWEIGHT_DECAY = 0.01\nNUM_EPOCHS = 3\nWARMUP_RATIO = 0.03\nGRADIENT_ACCUMULATION_STEPS = 8\n\n# LoRA configuration\nLORA_R = 16\nLORA_ALPHA = 32\nLORA_DROPOUT = 0.05\n\n# Debug mode for testing with smaller dataset\nDEBUG_MODE = False\nDEBUG_SAMPLE_SIZE = 100\n\nprint(f\"Using model: {MODEL_NAME}\")\nprint(f\"Max sequence length: {MAX_LENGTH}\")\nprint(f\"Batch size: {BATCH_SIZE}\")\nprint(f\"LoRA rank: {LORA_R}\")\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Function to load dataset with retry logic\ndef load_dataset_with_retry(dataset_id, max_retries=3, retry_delay=5):\n    \"\"\"Load a dataset with retry logic.\"\"\"\n    for attempt in range(max_retries):\n        try:\n            print(f\"Loading dataset (attempt {attempt+1}/{max_retries})...\")\n            data = load_dataset(dataset_id, trust_remote_code=True)\n            print(f\"Dataset loaded successfully with {len(data['train'])} examples\")\n            return data\n        except Exception as e:\n            print(f\"Error loading dataset (attempt {attempt+1}/{max_retries}): {e}\")\n            if attempt < max_retries - 1:\n                print(f\"Retrying in {retry_delay} seconds...\")\n                time.sleep(retry_delay)\n            else:\n                print(\"Maximum retries reached. Could not load dataset.\")\n                raise\n\n# Load the dataset with retry logic\ntry:\n    print(f\"Loading dataset: {DATASET_ID}\")\n    data = load_dataset_with_retry(DATASET_ID)\n    print(\"Dataset structure:\")\n    print(data)\n    \n    # If in debug mode, take a small sample of the dataset\n    if DEBUG_MODE and 'train' in data:\n        print(f\"DEBUG MODE: Sampling {DEBUG_SAMPLE_SIZE} examples from dataset\")\n        # Take a stratified sample if possible\n        data['train'] = data['train'].shuffle(seed=42).select(range(min(DEBUG_SAMPLE_SIZE, len(data['train']))))\n        print(f\"Reduced dataset size: {len(data['train'])} examples\")\n        \nexcept Exception as e:\n    print(f\"Fatal error loading dataset: {e}\")\n    raise\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Verify dataset structure and column names\ndef verify_dataset_structure(dataset):\n    \"\"\"Verify that the dataset has the expected structure and columns.\"\"\"\n    required_columns = ['repo_name', 'path', 'content']\n    if 'train' not in dataset:\n        print(\"WARNING: Dataset does not have a 'train' split.\")\n        return False\n    \n    missing_columns = [col for col in required_columns if col not in dataset['train'].column_names]\n    if missing_columns:\n        print(f\"WARNING: Dataset is missing required columns: {missing_columns}\")\n        return False\n    \n    print(\"Dataset structure verification passed.\")\n    return True\n\n# Verify dataset structure\ndataset_valid = verify_dataset_structure(data)\nif not dataset_valid:\n    print(\"Dataset structure is not as expected. Proceeding with caution.\")\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Load the Phi-3 tokenizer\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=MAX_LENGTH)\n    # Add padding token if it doesn't exist\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n    print(f\"Tokenizer type: {tokenizer.__class__.__name__}\")\nexcept Exception as e:\n    print(f\"Error loading tokenizer: {e}\")\n    raise",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def extract_file_type(path):\n    \"\"\"\n    Extract the file type/category based on the file path and naming conventions in Swift projects.\n    \n    Args:\n        path (str): The file path\n        \n    Returns:\n        int: The category label (0-5)\n    \"\"\"\n    path_lower = path.lower()\n    filename = path.split('/')[-1].lower()\n    \n    # Category 0: Models - Data structures and model definitions\n    if ('model' in path_lower or \n        'struct' in path_lower or \n        'entity' in path_lower or\n        'data' in path_lower and 'class' in path_lower):\n        return 0\n    \n    # Category 1: Views - UI related files\n    elif ('view' in path_lower or \n          'ui' in path_lower or \n          'screen' in path_lower or \n          'page' in path_lower or\n          'controller' in path_lower and 'view' in path_lower):\n        return 1\n    \n    # Category 2: Controllers - Application logic\n    elif ('controller' in path_lower or \n          'manager' in path_lower or \n          'coordinator' in path_lower or\n          'service' in path_lower):\n        return 2\n    \n    # Category 3: Utilities - Helper functions and extensions\n    elif ('util' in path_lower or \n          'helper' in path_lower or \n          'extension' in path_lower or\n          'common' in path_lower):\n        return 3\n    \n    # Category 4: Tests - Test files\n    elif ('test' in path_lower or \n          'spec' in path_lower or \n          'mock' in path_lower):\n        return 4\n    \n    # Category 5: Configuration - Package and configuration files\n    elif ('package.swift' in path_lower or \n          'config' in path_lower or \n          'settings' in path_lower or\n          'info.plist' in path_lower):\n        return 5\n    \n    # Default to category 3 (Utilities) if no clear category is found\n    return 3\n\n# Define category names for better readability\ncategory_names = {\n    0: \"Models\",\n    1: \"Views\",\n    2: \"Controllers\",\n    3: \"Utilities\",\n    4: \"Tests\",\n    5: \"Configuration\"\n}\n\n# Apply the function to create labels\ntry:\n    # Create a new column with the extracted labels\n    labeled_data = data['train'].map(lambda example: {\n        **example,\n        'label': extract_file_type(example['path'])\n    })\n    \n    # Count the distribution of labels\n    label_counts = collections.Counter(labeled_data['label'])\n    \n    print(\"Label distribution:\")\n    for label, count in sorted(label_counts.items()):\n        category_name = category_names.get(label, f\"Unknown-{label}\")\n        print(f\"Label {label} ({category_name}): {count} examples ({count/len(labeled_data)*100:.2f}%)\")\n    \n    # Get unique labels\n    unique_labels = sorted(label_counts.keys())\n    num_labels = len(unique_labels)\n    \n    print(f\"\\nTotal unique labels: {num_labels}\")\nexcept Exception as e:\n    print(f\"Error in data preparation: {e}\")\n    raise\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Split the data into train, validation, and test sets\ntry:\n    # Shuffle the data\n    shuffled_data = labeled_data.shuffle(seed=42)\n    \n    # Split into train (80%), validation (10%), and test (10%)\n    train_size = int(0.8 * len(shuffled_data))\n    val_size = int(0.1 * len(shuffled_data))\n    \n    train_data = shuffled_data.select(range(train_size))\n    val_data = shuffled_data.select(range(train_size, train_size + val_size))\n    test_data = shuffled_data.select(range(train_size + val_size, len(shuffled_data)))\n    \n    print(f\"Training set size: {len(train_data)}\")\n    print(f\"Training set label distribution: {collections.Counter(train_data['label'])}\")\n    print(f\"Validation set size: {len(val_data)}\")\n    print(f\"Validation set label distribution: {collections.Counter(val_data['label'])}\")\n    print(f\"Test set size: {len(test_data)}\")\n    print(f\"Test set label distribution: {collections.Counter(test_data['label'])}\")\nexcept Exception as e:\n    print(f\"Error splitting data: {e}\")\n    raise\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Create instruction-based prompts for the model\ndef create_instruction_prompt(example):\n    \"\"\"Convert a code example into an instruction-based prompt for language learning.\"\"\"\n    code = example['content']\n    label = example['label']\n    category = category_names.get(label, f\"Unknown-{label}\")\n    \n    # Create different types of prompts to help the model learn the language\n    prompt_types = [\n        # Explain code functionality\n        \"Explain what this Swift code does and how it works:\\n\\n\",\n        \n        # Identify patterns and features\n        \"Identify and explain the key Swift language features used in this code:\\n\\n\",\n        \n        # Complete or extend code\n        \"Complete or extend this Swift code with appropriate functionality:\\n\\n\",\n        \n        # Fix or improve code\n        \"Suggest improvements or best practices for this Swift code:\\n\\n\",\n        \n        # Understand code structure\n        f\"This is a Swift {category.lower()} file. Explain its structure and purpose:\\n\\n\",\n        \n        # Code generation tasks\n        \"Write a Swift function that accomplishes the same task as this code but more efficiently:\\n\\n\",\n        \n        # Language understanding\n        \"Explain the Swift syntax and language features demonstrated in this code:\\n\\n\",\n        \n        # Learning from examples\n        \"Study this Swift code example and explain what you can learn from it:\\n\\n\"\n    ]\n    \n    # Select a random prompt type\n    import random\n    instruction = random.choice(prompt_types)\n    \n    code_section = f\"```swift\\n{code}\\n```\\n\\n\"\n    \n    # Create the full prompt\n    prompt = instruction + code_section\n    \n    # Create a detailed response based on the prompt type and code category\n    if \"Explain what this Swift code does\" in instruction:\n        response = f\"This Swift code is a {category.lower()} file that \"\n        if category == \"Models\":\n            response += \"defines data structures and model objects. \"\n        elif category == \"Views\":\n            response += \"implements user interface components. \"\n        elif category == \"Controllers\":\n            response += \"manages application logic and coordinates between models and views. \"\n        elif category == \"Utilities\":\n            response += \"provides helper functions and extensions. \"\n        elif category == \"Tests\":\n            response += \"contains test cases to verify functionality. \"\n        elif category == \"Configuration\":\n            response += \"configures application settings and parameters. \"\n        \n        response += \"The code uses Swift syntax with \"\n        \n        # Add some language-specific details based on code content\n        if \"class\" in code:\n            response += \"class definitions, \"\n        if \"struct\" in code:\n            response += \"struct definitions, \"\n        if \"func\" in code:\n            response += \"function declarations, \"\n        if \"var\" in code:\n            response += \"variable declarations, \"\n        if \"let\" in code:\n            response += \"constant declarations, \"\n        if \"guard\" in code or \"if let\" in code:\n            response += \"optional unwrapping, \"\n        if \"extension\" in code:\n            response += \"extensions, \"\n        if \"protocol\" in code:\n            response += \"protocol implementations, \"\n            \n        # Remove trailing comma and space if present\n        if response.endswith(\", \"):\n            response = response[:-2] + \".\"\n        else:\n            response += \"various Swift features.\"\n    \n    elif \"Identify and explain the key Swift language features\" in instruction:\n        response = \"This Swift code demonstrates several key language features:\\n\\n\"\n        \n        # Add language features based on code content\n        features = []\n        if \"class\" in code:\n            features.append(\"1. **Classes**: Swift classes are reference types that support inheritance.\")\n        if \"struct\" in code:\n            features.append(\"1. **Structs**: Swift structs are value types that are copied when assigned or passed.\")\n        if \"protocol\" in code:\n            features.append(\"1. **Protocols**: Swift protocols define a blueprint of methods, properties, and requirements.\")\n        if \"extension\" in code:\n            features.append(\"1. **Extensions**: Swift extensions add new functionality to existing types.\")\n        if \"guard let\" in code:\n            features.append(\"1. **Guard statements**: Used for early returns and unwrapping optionals with cleaner code flow.\")\n        if \"if let\" in code:\n            features.append(\"1. **Optional binding**: Using if-let to safely unwrap optional values.\")\n        if \"enum\" in code:\n            features.append(\"1. **Enumerations**: Swift enums can contain methods and associated values.\")\n        if \"func\" in code and \"->\" in code:\n            features.append(\"1. **Functions with return types**: Swift functions with explicit return type declarations.\")\n        if \"var\" in code and \"?\" in code:\n            features.append(\"1. **Optional variables**: Variables that may contain a value or nil.\")\n        if \"let\" in code:\n            features.append(\"1. **Constants**: Immutable values defined with let.\")\n        \n        # If no specific features were identified, add a generic response\n        if not features:\n            features.append(\"1. **Swift syntax**: Basic Swift programming constructs and syntax.\")\n        \n        # Join the features with newlines\n        response += \"\\n\".join(features)\n    \n    # Return the prompt and response\n    return {\n        \"prompt\": prompt,\n        \"response\": response\n    }\n\n# Create instruction-based prompts for training\ntry:\n    # Apply the function to create prompts and responses\n    train_prompts = train_data.map(create_instruction_prompt)\n    val_prompts = val_data.map(create_instruction_prompt)\n    \n    # Print some examples\n    print(\"\\nExample prompts and responses:\")\n    for i in range(min(3, len(train_prompts))):\n        print(f\"\\nExample {i+1}:\")\n        print(f\"Prompt:\\n{train_prompts[i]['prompt'][:200]}...\")\n        print(f\"Response:\\n{train_prompts[i]['response'][:200]}...\")\n        print(\"-\" * 40)\n    \n    print(f\"\\nCreated {len(train_prompts)} training prompts and {len(val_prompts)} validation prompts\")\nexcept Exception as e:\n    print(f\"Error creating prompts: {e}\")\n    raise\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Tokenize the prompts and responses for training\ndef tokenize_prompt_response(example):\n    \"\"\"Tokenize a prompt and response pair for training.\"\"\"\n    # Format as a chat-like instruction\n    formatted_text = f\"<|user|>\\n{example['prompt']}\\n<|assistant|>\\n{example['response']}\"\n    \n    # Tokenize the text\n    tokenized = tokenizer(formatted_text, truncation=True, max_length=MAX_LENGTH, padding=\"max_length\")\n    \n    # Set the labels to be the same as the input_ids for causal language modeling\n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    \n    return tokenized\n\n# Apply tokenization to the datasets\ntry:\n    # Tokenize the training and validation datasets\n    tokenized_train = train_prompts.map(\n        tokenize_prompt_response,\n        remove_columns=train_prompts.column_names,\n        desc=\"Tokenizing training data\"\n    )\n    \n    tokenized_val = val_prompts.map(\n        tokenize_prompt_response,\n        remove_columns=val_prompts.column_names,\n        desc=\"Tokenizing validation data\"\n    )\n    \n    print(f\"Tokenized {len(tokenized_train)} training examples and {len(tokenized_val)} validation examples\")\n    \n    # Print an example of tokenized data\n    if len(tokenized_train) > 0:\n        example = tokenized_train[0]\n        print(\"\\nExample of tokenized data:\")\n        print(f\"Input IDs length: {len(example['input_ids'])}\")\n        print(f\"Attention mask length: {len(example['attention_mask'])}\")\n        print(f\"Labels length: {len(example['labels'])}\")\n        \n        # Decode a portion to verify\n        decoded = tokenizer.decode(example['input_ids'][:100])\n        print(f\"\\nDecoded first 100 tokens: {decoded}...\")\nexcept Exception as e:\n    print(f\"Error tokenizing data: {e}\")\n    raise\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Load the Phi-3 model with quantization for efficiency\ntry:\n    print(f\"Loading model: {MODEL_NAME}\")\n    \n    # Configure quantization for efficient training\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_use_double_quant=True\n    )\n    \n    # Load the model with quantization\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        quantization_config=quantization_config,\n        device_map=\"auto\",\n        trust_remote_code=True\n    )\n    \n    # Prepare the model for training with LoRA\n    model = prepare_model_for_kbit_training(model)\n    \n    # Configure LoRA for efficient fine-tuning\n    lora_config = LoraConfig(\n        r=LORA_R,\n        lora_alpha=LORA_ALPHA,\n        lora_dropout=LORA_DROPOUT,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",  # Using string literal instead of enum\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n    )\n    \n    # Apply LoRA to the model\n    model = get_peft_model(model, lora_config)\n    \n    # Print model information\n    print(f\"Model loaded: {model.__class__.__name__}\")\n    print(f\"Model parameters: {model.num_parameters():,} total, {model.num_parameters(True):,} trainable\")\n    print(f\"Training {100 * model.num_parameters(True) / model.num_parameters():.2f}% of parameters\")\n    \n    # Clean up memory\n    cleanup_memory()\nexcept Exception as e:\n    print(f\"Error loading model: {e}\")\n    raise\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./phi3_swift_results\",\n    num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    warmup_ratio=WARMUP_RATIO,\n    weight_decay=WEIGHT_DECAY,\n    learning_rate=LEARNING_RATE,\n    # Removed evaluation_strategy and eval_steps as they're not supported in this version\n    # Removed save_strategy and save_steps as they might not be supported\n    # Removed load_best_model_at_end as it depends on evaluation_strategy\n    logging_dir=\"./logs\",\n    logging_steps=100,\n    save_total_limit=3,\n    fp16=torch.cuda.is_available(),\n    report_to=\"none\",\n    remove_unused_columns=False,\n    push_to_hub=False,\n    disable_tqdm=False,\n    dataloader_num_workers=2,\n    dataloader_pin_memory=True,\n    group_by_length=True  # Group similar length sequences for efficiency\n)\n\n# Define early stopping callback\nearly_stopping_callback = EarlyStoppingCallback(\n    early_stopping_patience=3,\n    early_stopping_threshold=0.01\n)\n\n# Create data collator for language modeling\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False  # We're doing causal language modeling, not masked language modeling\n)\n\n# Create trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    callbacks=[early_stopping_callback]\n)\n\nprint(\"Training setup complete\")",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Function to monitor system resources during training\ndef monitor_resources():\n    process = psutil.Process(os.getpid())\n    memory_info = process.memory_info()\n    mem = psutil.virtual_memory()\n    cpu_percent = psutil.cpu_percent(interval=0.1)\n    \n    print(f\"\\nSystem Resources:\")\n    print(f\"CPU Usage: {cpu_percent}%\")\n    print(f\"Process Memory: {memory_info.rss / 1024 / 1024:.2f} MB\")\n    print(f\"System Memory: {mem.percent}% used, {mem.available / 1024 / 1024:.2f} MB available\\n\")\n\n# Run training with more detailed monitoring\ntry:\n    print(\"Starting training...\")\n    \n    # Monitor resources before training\n    print(\"Resources before training:\")\n    monitor_resources()\n    \n    # Start training with a timeout\n    start_time = time.time()\n    \n    # Run training\n    train_result = trainer.train()\n    \n    # Monitor resources after training\n    print(\"Resources after training:\")\n    monitor_resources()\n    \n    # Print training results\n    print(f\"Training completed in {train_result.metrics['train_runtime']:.2f} seconds\")\n    print(f\"Training loss: {train_result.metrics['train_loss']:.4f}\")\n    \n    # Save the model\n    trainer.save_model(\"./phi3_swift_model\")\n    print(\"Model saved to ./phi3_swift_model\")\n    \n    # Clean up memory\n    cleanup_memory()\n    \nexcept Exception as e:\n    print(f\"Error during training: {e}\")\n    \n    # Print stack trace for debugging\n    import traceback\n    traceback.print_exc()\n    \n    # Monitor resources after error\n    print(\"Resources after error:\")\n    monitor_resources()\n    \n    raise\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Test the model with Swift code examples\ntry:\n    print(\"Testing the model with Swift code examples...\")\n    \n    # Function to generate responses for test examples\n    def generate_response(prompt):\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n        with torch.no_grad():\n            outputs = model.generate(\n                inputs.input_ids,\n                max_new_tokens=200,\n                temperature=0.7,\n                top_p=0.9,\n                do_sample=True,\n                pad_token_id=tokenizer.pad_token_id,\n                eos_token_id=tokenizer.eos_token_id\n            )\n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        # Extract just the assistant's response\n        if \"<|assistant|>\" in response:\n            response = response.split(\"<|assistant|>\")[-1].strip()\n        return response\n    \n    # Test prompts for different Swift language tasks\n    test_prompts = [\n        # Explain Swift syntax\n        \"<|user|>\\nExplain the key features of Swift's optional unwrapping syntax:\\n\\n```swift\\nfunc processName(_ name: String?) {\\n    guard let unwrappedName = name else {\\n        print(\"No name provided\")\\n        return\\n    }\\n    print(\"Hello, \\(unwrappedName)!\")\\n}\\n```\\n<|assistant|>\",\n        \n        # Code completion\n        \"<|user|>\\nComplete this Swift function that calculates the factorial of a number:\\n\\n```swift\\nfunc factorial(_ n: Int) -> Int {\\n    // Add implementation here\\n}\\n```\\n<|assistant|>\",\n        \n        # Debugging help\n        \"<|user|>\\nWhat's wrong with this Swift code and how can I fix it?\\n\\n```swift\\nclass Person {\\n    var name: String\\n    var age: Int\\n    \\n    func greet() {\\n        print(\"Hello, my name is \\(name) and I am \\(age) years old.\")\\n    }\\n}\\n\\nlet person = Person()\\nperson.greet()\\n```\\n<|assistant|>\",\n        \n        # Swift best practices\n        \"<|user|>\\nExplain Swift best practices for error handling:\\n<|assistant|>\"\n    ]\n    \n    # Generate and print responses\n    for i, prompt in enumerate(test_prompts):\n        print(f\"\\nTest {i+1}:\\n{'-'*40}\")\n        print(f\"Prompt: {prompt.split('<|assistant|>')[0].replace('<|user|>', '')}\")\n        response = generate_response(prompt)\n        print(f\"\\nResponse:\\n{response}\\n\")\n    \n    print(\"\\nTesting complete\")\nexcept Exception as e:\n    print(f\"Error during testing: {e}\")\n    import traceback\n    traceback.print_exc()\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  }
 ]
}