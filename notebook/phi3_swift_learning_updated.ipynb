{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Training Phi-3-mini-128k-instruct on Swift Code (TPU Fixed Version)\n\nThis notebook fine-tunes the Microsoft Phi-3-mini-128k-instruct model on Swift code to learn the Swift programming language. The notebook supports training on TPU, GPU, or CPU with appropriate optimizations for each hardware type. This version fixes TPU compatibility issues."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required libraries with specific versions for compatibility\n!pip install transformers==4.38.2 datasets==2.16.1 evaluate==0.4.1 torch==2.1.2 scikit-learn==1.4.0 tqdm==4.66.1 accelerate==0.27.2 peft==0.7.1 bitsandbytes==0.41.3 psutil==5.9.8\n\n# Install TPU-specific libraries if needed\ntry:\n    import torch_xla\n    print(\"PyTorch XLA already installed\")\nexcept ImportError:\n    print(\"Installing PyTorch XLA for TPU support...\")\n    !pip install cloud-tpu-client torch_xla\n    \n# Verify installations\nprint(\"\\nVerifying installations:\")\n!python -c \"import torch; print(f'PyTorch version: {torch.__version__}')\"\n!python -c \"import transformers; print(f'Transformers version: {transformers.__version__}')\"\n!python -c \"import peft; print(f'PEFT version: {peft.__version__}')\"\n!python -c \"import bitsandbytes as bnb; print(f'BitsAndBytes version: {bnb.__version__}')\"\ntry:\n    !python -c \"import torch_xla; print(f'PyTorch XLA version: {torch_xla.__version__}')\"\nexcept:\n    print(\"PyTorch XLA not installed (only needed for TPU)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Important: Comprehensive imports for Phi-3 training with GPU/TPU support\n# System and utility imports\nimport os\nimport sys\nimport json\nimport time\nimport gc\nimport re\nimport logging\nimport traceback\nimport collections\nimport warnings\nimport psutil  # For memory monitoring\nfrom typing import Dict, List, Optional, Union, Tuple, Any\nfrom tqdm.auto import tqdm\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Data processing imports\nimport random\nimport numpy as np\nimport pandas as pd\n\n# PyTorch imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler\n\n# HuggingFace imports\nfrom datasets import load_dataset, ClassLabel, Dataset as HFDataset, DatasetDict\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM,\n    Trainer, \n    TrainingArguments,\n    set_seed,\n    DataCollatorForLanguageModeling,\n    EarlyStoppingCallback,\n    BitsAndBytesConfig,\n    TrainerCallback,\n    TrainerState,\n    TrainerControl,\n    TrainingArguments,\n    logging as transformers_logging\n)\n\n# PEFT (Parameter-Efficient Fine-Tuning) imports\nfrom peft import (\n    LoraConfig, \n    get_peft_model, \n    prepare_model_for_kbit_training,\n    PeftModel,\n    PeftConfig\n    # TaskType is not imported directly to avoid compatibility issues\n)\n\n# BitsAndBytes for quantization\nfrom bitsandbytes.nn import LinearNF4, Linear8bitLt\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[logging.StreamHandler(sys.stdout)]\n)\nlogger = logging.getLogger(__name__)\ntransformers_logging.set_verbosity_info()\n\n# Set a seed for reproducibility\nset_seed(42)\n\n# Custom callback for detailed training monitoring\nclass ResourceMonitorCallback(TrainerCallback):\n    \"\"\"Callback to monitor system resources during training\"\"\"\n    def __init__(self, log_every=100):\n        self.log_every = log_every\n        self.step = 0\n    \n    def on_step_end(self, args, state, control, **kwargs):\n        self.step += 1\n        if self.step % self.log_every == 0:\n            process = psutil.Process(os.getpid())\n            memory_info = process.memory_info()\n            mem = psutil.virtual_memory()\n            cpu_percent = psutil.cpu_percent(interval=0.1)\n            \n            logger.info(f\"Step {self.step} - CPU: {cpu_percent}% | RAM: {memory_info.rss / 1024 / 1024:.2f} MB | System RAM: {mem.percent}% used\")\n            \n            # GPU monitoring if available\n            if torch.cuda.is_available():\n                for i in range(torch.cuda.device_count()):\n                    gpu_mem_alloc = torch.cuda.memory_allocated(i) / 1024 / 1024\n                    gpu_mem_reserved = torch.cuda.memory_reserved(i) / 1024 / 1024\n                    logger.info(f\"GPU {i}: Allocated: {gpu_mem_alloc:.2f} MB | Reserved: {gpu_mem_reserved:.2f} MB\")\n\n# Enhanced memory management function with detailed reporting\ndef cleanup_memory():\n    \"\"\"Force garbage collection and clear hardware accelerator cache if available.\"\"\"\n    # Get memory usage before cleanup\n    before = psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2\n    \n    # Perform cleanup\n    gc.collect()\n    \n    # Clear GPU cache if available\n    if torch.cuda.is_available():\n        for i in range(torch.cuda.device_count()):\n            gpu_before = torch.cuda.memory_allocated(i) / 1024 / 1024\n            torch.cuda.empty_cache()\n            gpu_after = torch.cuda.memory_allocated(i) / 1024 / 1024\n            print(f\"GPU {i} memory cleaned: {gpu_before:.2f} MB → {gpu_after:.2f} MB, Freed: {gpu_before - gpu_after:.2f} MB\")\n    \n    # Try to clear TPU cache if available\n    try:\n        import torch_xla.core.xla_model as xm\n        xm.mark_step()\n        print(\"TPU cache cleared with mark_step()\")\n    except (ImportError, NameError):\n        pass  # TPU not available\n    \n    # Get memory usage after cleanup\n    after = psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2\n    print(f\"Process memory cleaned: {before:.2f} MB → {after:.2f} MB, Freed: {before - after:.2f} MB\")\n    \n    # Print system memory info\n    mem = psutil.virtual_memory()\n    print(f\"System memory: {mem.percent}% used, {mem.available / 1024 / 1024:.2f} MB available\")\n    \n    return before - after  # Return amount of memory freed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check for available hardware accelerators (GPU, TPU)\nimport torch\nimport os\n\n# Function to detect and configure the device\ndef setup_device():\n    # Check for TPU\n    try:\n        import torch_xla\n        import torch_xla.core.xla_model as xm\n        print(\"TPU is available. Setting up TPU device...\")\n        device = xm.xla_device()\n        print(f\"Using TPU: {device}\")\n        return device, \"tpu\"\n    except (ImportError, EnvironmentError):\n        print(\"TPU not available or not properly configured.\")\n    \n    # Check for GPU\n    if torch.cuda.is_available():\n        device = torch.device('cuda')\n        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n        print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n        return device, \"gpu\"\n    \n    # Fallback to CPU\n    print(\"No GPU or TPU detected. Using CPU - Note: Training will be much slower on CPU\")\n    return torch.device('cpu'), \"cpu\"\n\n# Set up device\ndevice, device_type = setup_device()\n\n# Set random seeds for reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif device_type == \"gpu\":\n    torch.cuda.manual_seed_all(SEED)\n\n# Set environment variables for better performance\nif device_type == \"gpu\":\n    # For GPU\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use first GPU by default\n    torch.backends.cudnn.benchmark = True     # Optimize for fixed input sizes\n    torch.backends.cuda.matmul.allow_tf32 = True  # Allow TF32 on Ampere GPUs\n    torch.backends.cudnn.allow_tf32 = True\n    print(\"CUDA optimizations enabled\")\nelif device_type == \"tpu\":\n    # For TPU\n    os.environ[\"XLA_USE_BF16\"] = \"1\"  # Enable bfloat16 for better performance on TPU\n    print(\"TPU optimizations enabled\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define model and training parameters\nMODEL_NAME = \"microsoft/Phi-3-mini-128k-instruct\"  # The model we're fine-tuning\nMAX_LENGTH = 4096  # Maximum sequence length\nBATCH_SIZE = 4  # Batch size for training\nLEARNING_RATE = 2e-5  # Learning rate\nWEIGHT_DECAY = 0.01  # Weight decay for regularization\nNUM_EPOCHS = 3  # Number of training epochs\nGRADIENT_ACCUMULATION_STEPS = 8  # Gradient accumulation steps\nWARMUP_RATIO = 0.03  # Warmup ratio\n\n# LoRA parameters\nLORA_R = 16  # LoRA attention dimension\nLORA_ALPHA = 32  # LoRA alpha parameter\nLORA_DROPOUT = 0.05  # Dropout probability for LoRA layers\n\n# Print configuration\nprint(\"\\n\" + \"=\" * 50)\nprint(\"MODEL CONFIGURATION\")\nprint(\"=\" * 50)\nprint(f\"Model: {MODEL_NAME}\")\nprint(f\"Max Length: {MAX_LENGTH}\")\nprint(f\"Batch Size: {BATCH_SIZE}\")\nprint(f\"Learning Rate: {LEARNING_RATE}\")\nprint(f\"Weight Decay: {WEIGHT_DECAY}\")\nprint(f\"Epochs: {NUM_EPOCHS}\")\nprint(f\"Gradient Accumulation Steps: {GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"LoRA r: {LORA_R}, alpha: {LORA_ALPHA}, dropout: {LORA_DROPOUT}\")\nprint(f\"Device: {device_type.upper()}\")\nprint(\"=\" * 50 + \"\\n\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the dataset\nprint(\"Loading Swift code dataset...\")\ntry:\n    # Load the dataset\n    dataset = load_dataset(\"mvasiliniuc/iva-swift-codeint\")\n    print(f\"Dataset loaded successfully with {len(dataset['train'])} training examples\")\n    \n    # Print dataset information\n    print(\"\\nDataset structure:\")\n    print(dataset)\n    \n    # Get a sample to understand the data structure\n    sample = dataset['train'][0]\n    print(\"\\nSample data point:\")\n    for key, value in sample.items():\n        if isinstance(value, str) and len(value) > 100:\n            print(f\"{key}: {value[:100]}...\")\n        else:\n            print(f\"{key}: {value}\")\n    \n    # Get label distribution\n    if 'label' in sample:\n        labels = [example['label'] for example in dataset['train']]\n        label_counts = collections.Counter(labels)\n        print(\"\\nLabel distribution:\")\n        for label, count in label_counts.items():\n            print(f\"Label {label}: {count} examples ({count/len(dataset['train'])*100:.2f}%)\")\n    \n    # Create category names mapping\n    category_names = {\n        0: \"Models\",\n        1: \"Views\",\n        2: \"Controllers\",\n        3: \"Utilities\",\n        4: \"Tests\",\n        5: \"Configuration\"\n    }\n    \n    # Split the dataset into train, validation, and test sets\n    if 'validation' not in dataset or 'test' not in dataset:\n        print(\"\\nSplitting dataset into train, validation, and test sets...\")\n        splits = dataset['train'].train_test_split(test_size=0.2, seed=SEED)\n        train_data = splits['train']\n        \n        # Further split the test set into validation and test\n        test_splits = splits['test'].train_test_split(test_size=0.5, seed=SEED)\n        val_data = test_splits['train']\n        test_data = test_splits['test']\n        \n        print(f\"Train set: {len(train_data)} examples\")\n        print(f\"Validation set: {len(val_data)} examples\")\n        print(f\"Test set: {len(test_data)} examples\")\n    else:\n        train_data = dataset['train']\n        val_data = dataset['validation']\n        test_data = dataset['test']\n        print(f\"Using existing splits: Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n    \n    # Clean up memory\n    cleanup_memory()\n    \nexcept Exception as e:\n    print(f\"Error loading dataset: {e}\")\n    traceback.print_exc()\n    raise"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load tokenizer\nprint(\"\\nLoading tokenizer...\")\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    \n    # Ensure the tokenizer has padding and EOS tokens\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    \n    print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n    print(f\"Vocabulary size: {len(tokenizer)}\")\n    print(f\"Model max length: {tokenizer.model_max_length}\")\n    print(f\"Padding token: {tokenizer.pad_token} (id: {tokenizer.pad_token_id})\")\n    print(f\"EOS token: {tokenizer.eos_token} (id: {tokenizer.eos_token_id})\")\n    \n    # Test tokenization on a sample\n    sample_text = \"func hello() {\\n    print(\\\"Hello, Swift!\\\")\\n}\"\n    tokens = tokenizer(sample_text, return_tensors=\"pt\")\n    print(f\"\\nSample text tokenized to {len(tokens.input_ids[0])} tokens\")\n    \nexcept Exception as e:\n    print(f\"Error loading tokenizer: {e}\")\n    traceback.print_exc()\n    raise"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create instruction-based prompts for the model\ndef create_instruction_prompt(example):\n    \"\"\"Convert a code example into an instruction-based prompt for language learning.\"\"\"\n    code = example['content']\n    label = example['label']\n    category = category_names.get(label, f\"Unknown-{label}\")\n    \n    # Create different types of prompts to help the model learn the language\n    prompt_types = [\n        # Explain code functionality\n        \"Explain what this Swift code does and how it works:\\n\\n\",\n        \n        # Identify patterns and features\n        \"Identify and explain the key Swift language features used in this code:\\n\\n\",\n        \n        # Complete or extend code\n        \"Complete or extend this Swift code with appropriate functionality:\\n\\n\",\n        \n        # Fix or improve code\n        \"Suggest improvements or best practices for this Swift code:\\n\\n\",\n        \n        # Understand code structure\n        f\"This is a Swift {category.lower()} file. Explain its structure and purpose:\\n\\n\",\n        \n        # Code generation tasks\n        \"Write a Swift function that accomplishes the same task as this code but more efficiently:\\n\\n\",\n        \n        # Language understanding\n        \"Explain the Swift syntax and language features demonstrated in this code:\\n\\n\",\n        \n        # Learning from examples\n        \"Study this Swift code example and explain what you can learn from it:\\n\\n\"\n    ]\n    \n    # Select a random prompt type\n    instruction = random.choice(prompt_types)\n    \n    code_section = f\"```swift\\n{code}\\n```\\n\\n\"\n    \n    # Create the full prompt\n    prompt = instruction + code_section\n    \n    # Create a detailed response based on the prompt type and code category\n    if \"Explain what this Swift code does\" in instruction:\n        response = f\"This Swift code is a {category.lower()} file that \"\n        if category == \"Models\":\n            response += \"defines data structures and model objects. \"\n        elif category == \"Views\":\n            response += \"implements user interface components. \"\n        elif category == \"Controllers\":\n            response += \"manages application logic and coordinates between models and views. \"\n        elif category == \"Utilities\":\n            response += \"provides helper functions and extensions. \"\n        elif category == \"Tests\":\n            response += \"contains test cases to verify functionality. \"\n        elif category == \"Configuration\":\n            response += \"configures application settings and parameters. \"\n        \n        response += \"The code uses Swift syntax with \"\n        \n        # Add some language-specific details based on code content\n        if \"class\" in code:\n            response += \"class definitions, \"\n        if \"struct\" in code:\n            response += \"struct definitions, \"\n        if \"func\" in code:\n            response += \"function declarations, \"\n        if \"var\" in code:\n            response += \"variable declarations, \"\n        if \"let\" in code:\n            response += \"constant declarations, \"\n        if \"guard\" in code or \"if let\" in code:\n            response += \"optional unwrapping, \"\n        if \"extension\" in code:\n            response += \"extensions, \"\n        if \"protocol\" in code:\n            response += \"protocol implementations, \"\n            \n        # Remove trailing comma and space if present\n        if response.endswith(\", \"):\n            response = response[:-2] + \".\"\n        else:\n            response += \"various Swift features.\"\n    \n    elif \"Identify and explain the key Swift language features\" in instruction:\n        response = \"This Swift code demonstrates several key language features:\\n\\n\"\n        \n        # Add language features based on code content\n        features = []\n        if \"class\" in code:\n            features.append(\"1. **Classes**: Swift classes are reference types that support inheritance and reference counting.\")\n        if \"struct\" in code:\n            features.append(\"1. **Structs**: Swift structs are value types that are copied when assigned or passed as arguments.\")\n        if \"protocol\" in code:\n            features.append(\"1. **Protocols**: Similar to interfaces in other languages, protocols define a blueprint of methods, properties, and requirements.\")\n        if \"extension\" in code:\n            features.append(\"1. **Extensions**: Allow adding new functionality to existing types without modifying the original code.\")\n        if \"enum\" in code:\n            features.append(\"1. **Enumerations**: Define a common type for a group of related values and enable you to work with those values in a type-safe way.\")\n        if \"guard\" in code:\n            features.append(\"1. **Guard statements**: Early exit mechanism that improves code readability by handling edge cases early.\")\n        if \"if let\" in code or \"guard let\" in code:\n            features.append(\"1. **Optional binding**: Safely unwraps optional values using if let or guard let syntax.\")\n        if \"func\" in code:\n            features.append(\"1. **Functions**: First-class citizens in Swift that can be passed around and used like any other value.\")\n        if \"closure\" in code or \"{\" in code and \"in\" in code:\n            features.append(\"1. **Closures**: Self-contained blocks of functionality that can be passed around and used in your code.\")\n        \n        # If no specific features were identified, provide a generic response\n        if not features:\n            features = [\"1. **Swift syntax**: The code demonstrates standard Swift syntax and conventions.\",\n                       \"2. **Type safety**: Swift's strong type system is evident in the variable declarations.\",\n                       \"3. **Object-oriented programming**: The code follows Swift's object-oriented programming principles.\"]\n        \n        # Fix numbering\n        for i, feature in enumerate(features):\n            features[i] = feature.replace(\"1.\", f\"{i+1}.\")\n        \n        response += \"\\n\".join(features)\n    \n    elif \"Complete or extend this Swift code\" in instruction or \"Write a Swift function\" in instruction:\n        # For code generation tasks, provide a thoughtful response about how to approach the task\n        response = f\"To extend this Swift {category.lower()} code, I would consider the following approach:\\n\\n\"\n        \n        if category == \"Models\":\n            response += \"1. Add additional properties to capture more data attributes\\n\"\n            response += \"2. Implement Codable protocol for easy JSON serialization\\n\"\n            response += \"3. Add validation methods to ensure data integrity\\n\"\n            response += \"4. Include computed properties for derived values\\n\\n\"\n            response += \"Here's an implementation example:\\n\\n```swift\\n\"\n            \n            if \"struct\" in code:\n                response += \"// Extension to add Codable conformance\\nextension MyStruct: Codable {\\n    // Codable implementation\\n}\\n\\n\"\n                response += \"// Add validation method\\nextension MyStruct {\\n    func validate() -> Bool {\\n        // Validation logic\\n        return true\\n    }\\n}\\n\"\n            else:\n                response += \"// Example extension or additional functionality\\n// that would be appropriate for this model\\n\"\n            \n            response += \"```\"\n            \n        elif category == \"Views\":\n            response += \"1. Add UI customization options\\n\"\n            response += \"2. Implement additional user interaction handlers\\n\"\n            response += \"3. Add accessibility support\\n\"\n            response += \"4. Implement view lifecycle methods\\n\\n\"\n            response += \"Here's an implementation example:\\n\\n```swift\\n\"\n            response += \"// Example extension or additional functionality\\n// that would be appropriate for this view\\n\"\n            response += \"```\"\n            \n        else:\n            response += \"1. Add error handling to make the code more robust\\n\"\n            response += \"2. Implement additional helper methods\\n\"\n            response += \"3. Add documentation comments to improve code readability\\n\"\n            response += \"4. Consider performance optimizations where appropriate\\n\\n\"\n            response += \"Here's an implementation example:\\n\\n```swift\\n\"\n            response += \"// Example extension or additional functionality\\n// that would be appropriate for this code\\n\"\n            response += \"```\"\n    \n    else:\n        # Generic response for other prompt types\n        response = f\"This Swift code demonstrates typical patterns used in {category.lower()} files. \"\n        response += \"It follows Swift language conventions and showcases proper syntax for defining \"\n        \n        if category == \"Models\":\n            response += \"data structures with properties and methods. Swift models typically use structs for value semantics or classes when reference semantics are needed. The code demonstrates Swift's strong typing system and property declarations.\"\n        elif category == \"Views\":\n            response += \"UI components with layout and interaction logic. Swift views often use UIKit or SwiftUI frameworks, with clear separation of UI elements and their behaviors. The code shows how Swift handles user interface components and event responses.\"\n        elif category == \"Controllers\":\n            response += \"application logic and coordination between components. Controllers in Swift manage the flow of data between models and views, implementing business logic and handling user interactions. The code demonstrates Swift's approach to application architecture.\"\n        elif category == \"Utilities\":\n            response += \"helper functions and extensions to enhance functionality. Swift utilities often leverage the language's powerful extension capabilities to add functionality to existing types. The code shows how Swift can be extended and customized through utility functions.\"\n        elif category == \"Tests\":\n            response += \"test cases with setup, execution, and verification steps. Swift tests typically use XCTest framework with arrange-act-assert pattern. The code demonstrates Swift's approach to unit testing and verification.\"\n        elif category == \"Configuration\":\n            response += \"application settings and configuration parameters. Swift configuration files often define constants, environment settings, and application parameters. The code shows how Swift handles application configuration and settings management.\"\n    \n    # Combine prompt and response for instruction tuning\n    full_text = f\"<|user|>\\n{prompt}\\n<|assistant|>\\n{response}\\n\"\n    \n    return {\n        \"text\": full_text,\n        \"prompt\": prompt,\n        \"response\": response,\n        \"label\": label,\n        \"category\": category\n    }\n\n# Apply the function to create instruction-based datasets\ntry:\n    # Create instruction datasets\n    print(\"\\nCreating instruction-based prompts for training...\")\n    train_instructions = train_data.map(create_instruction_prompt)\n    val_instructions = val_data.map(create_instruction_prompt)\n    test_instructions = test_data.map(create_instruction_prompt)\n    \n    # Print an example to verify\n    print(\"\\nExample instruction prompt:\")\n    print(\"-\" * 80)\n    print(train_instructions[0]['text'])\n    print(\"-\" * 80)\n    \n    print(f\"Created {len(train_instructions)} training instructions\")\n    print(f\"Created {len(val_instructions)} validation instructions\")\n    print(f\"Created {len(test_instructions)} test instructions\")\nexcept Exception as e:\n    print(f\"Error creating instruction prompts: {e}\")\n    traceback.print_exc()\n    raise"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Tokenize the datasets\nprint(\"\\nTokenizing datasets...\")\n\n# Function to tokenize the text\ndef tokenize_function(examples):\n    # Tokenize the full instruction text\n    tokenized = tokenizer(\n        examples[\"text\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=MAX_LENGTH,\n        return_tensors=None  # Return Python lists instead of PyTorch tensors\n    )\n    return tokenized\n\ntry:\n    # Apply tokenization to the datasets\n    print(\"Tokenizing training set...\")\n    tokenized_train = train_instructions.map(\n        tokenize_function,\n        batched=True,\n        remove_columns=[\"content\", \"label\", \"text\", \"prompt\", \"response\", \"category\"]\n    )\n    \n    print(\"Tokenizing validation set...\")\n    tokenized_val = val_instructions.map(\n        tokenize_function,\n        batched=True,\n        remove_columns=[\"content\", \"label\", \"text\", \"prompt\", \"response\", \"category\"]\n    )\n    \n    print(\"Tokenizing test set...\")\n    tokenized_test = test_instructions.map(\n        tokenize_function,\n        batched=True,\n        remove_columns=[\"content\", \"label\", \"text\", \"prompt\", \"response\", \"category\"]\n    )\n    \n    # Print tokenized dataset information\n    print(f\"\\nTokenized training set: {len(tokenized_train)} examples\")\n    print(f\"Tokenized validation set: {len(tokenized_val)} examples\")\n    print(f\"Tokenized test set: {len(tokenized_test)} examples\")\n    \n    # Print a sample of tokenized data\n    print(\"\\nSample tokenized data:\")\n    sample_idx = 0\n    print(f\"Input IDs (first 10): {tokenized_train[sample_idx]['input_ids'][:10]}\")\n    print(f\"Attention Mask (first 10): {tokenized_train[sample_idx]['attention_mask'][:10]}\")\n    print(f\"Total tokens: {sum(tokenized_train[sample_idx]['attention_mask'])}\")\n    \n    # Clean up memory\n    cleanup_memory()\n    \nexcept Exception as e:\n    print(f\"Error tokenizing datasets: {e}\")\n    traceback.print_exc()\n    raise"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "try:\n    # Configure model loading based on device type\n    print(\"\\n\" + \"=\" * 50)\n    print(f\"HARDWARE CONFIGURATION: {device_type.upper()}\")\n    print(\"=\" * 50)\n    \n    if device_type == \"tpu\":\n        # TPU-specific configuration\n        print(\"\\n[TPU SETUP] Configuring model for TPU training...\")\n        print(f\"[TPU SETUP] Using device: {device}\")\n        \n        # Import TPU-specific modules\n        import torch_xla.core.xla_model as xm\n        import torch_xla.distributed.parallel_loader as pl\n        \n        # Get TPU core count and worker information\n        tpu_cores = xm.xrt_world_size()\n        tpu_worker = xm.get_ordinal()\n        print(f\"[TPU SETUP] TPU cores available: {tpu_cores}\")\n        print(f\"[TPU SETUP] Current TPU worker: {tpu_worker}\")\n        \n        # TPUs work better with bf16 precision\n        print(\"[TPU SETUP] Loading model with bfloat16 precision...\")\n        model = AutoModelForCausalLM.from_pretrained(\n            MODEL_NAME,\n            torch_dtype=torch.bfloat16,  # Use bfloat16 for TPU\n            trust_remote_code=True\n        )\n        print(f\"[TPU SETUP] Model loaded: {MODEL_NAME}\")\n        \n        # Configure LoRA for TPU\n        print(\"[TPU SETUP] Configuring LoRA parameters...\")\n        lora_config = LoraConfig(\n            r=LORA_R,\n            lora_alpha=LORA_ALPHA,\n            lora_dropout=LORA_DROPOUT,\n            bias=\"none\",\n            task_type=\"CAUSAL_LM\",\n            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n        )\n        print(f\"[TPU SETUP] LoRA config: rank={LORA_R}, alpha={LORA_ALPHA}, dropout={LORA_DROPOUT}\")\n        \n        # Apply LoRA to the model\n        print(\"[TPU SETUP] Applying LoRA to model...\")\n        model = get_peft_model(model, lora_config)\n        \n        # Move model to TPU device\n        print(\"[TPU SETUP] Moving model to TPU device...\")\n        model = model.to(device)\n        \n        print(\"[TPU SETUP] TPU configuration complete\")\n        \n    elif device_type == \"gpu\":\n        # GPU-specific configuration with quantization\n        print(\"\\n[GPU SETUP] Configuring model for GPU training with quantization...\")\n        \n        # Get GPU information\n        gpu_count = torch.cuda.device_count()\n        gpu_name = torch.cuda.get_device_name(0)\n        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n        print(f\"[GPU SETUP] Using GPU: {gpu_name}\")\n        print(f\"[GPU SETUP] GPU count: {gpu_count}\")\n        print(f\"[GPU SETUP] GPU memory: {gpu_memory:.2f} GB\")\n        \n        # Configure quantization for efficient GPU training\n        print(\"[GPU SETUP] Configuring 4-bit quantization...\")\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.float16,\n            bnb_4bit_use_double_quant=True\n        )\n        print(\"[GPU SETUP] Quantization type: NF4 (4-bit)\")\n        \n        # Load the Phi-3 model for causal language modeling\n        print(f\"[GPU SETUP] Loading model: {MODEL_NAME}\")\n        model = AutoModelForCausalLM.from_pretrained(\n            MODEL_NAME,\n            quantization_config=bnb_config,\n            device_map=\"auto\",\n            trust_remote_code=True\n        )\n        print(\"[GPU SETUP] Model loaded successfully\")\n        \n        # Prepare the model for training\n        print(\"[GPU SETUP] Preparing model for k-bit training...\")\n        model = prepare_model_for_kbit_training(model)\n        \n        # Configure LoRA\n        print(\"[GPU SETUP] Configuring LoRA parameters...\")\n        lora_config = LoraConfig(\n            r=LORA_R,\n            lora_alpha=LORA_ALPHA,\n            lora_dropout=LORA_DROPOUT,\n            bias=\"none\",\n            task_type=\"CAUSAL_LM\",\n            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n        )\n        print(f\"[GPU SETUP] LoRA config: rank={LORA_R}, alpha={LORA_ALPHA}, dropout={LORA_DROPOUT}\")\n        \n        # Apply LoRA to the model\n        print(\"[GPU SETUP] Applying LoRA to model...\")\n        model = get_peft_model(model, lora_config)\n        \n        print(\"[GPU SETUP] GPU configuration complete\")\n        \n    else:\n        # CPU fallback - use smaller model or minimal settings\n        print(\"\\n[CPU SETUP] Configuring model for CPU training (minimal configuration)...\")\n        print(\"[CPU SETUP] WARNING: Training on CPU will be very slow!\")\n        \n        # Load the model with minimal settings for CPU\n        print(f\"[CPU SETUP] Loading model: {MODEL_NAME}\")\n        model = AutoModelForCausalLM.from_pretrained(\n            MODEL_NAME,\n            torch_dtype=torch.float32,  # Use float32 for CPU\n            trust_remote_code=True,\n            low_cpu_mem_usage=True\n        )\n        print(\"[CPU SETUP] Model loaded with float32 precision\")\n        \n        # Configure LoRA with smaller rank for CPU\n        print(\"[CPU SETUP] Configuring LoRA with reduced parameters for CPU...\")\n        lora_config = LoraConfig(\n            r=8,  # Smaller rank for CPU\n            lora_alpha=16,\n            lora_dropout=LORA_DROPOUT,\n            bias=\"none\",\n            task_type=\"CAUSAL_LM\",\n            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n        )\n        print(\"[CPU SETUP] Using reduced LoRA rank (r=8) for CPU efficiency\")\n        \n        # Apply LoRA to the model\n        print(\"[CPU SETUP] Applying LoRA to model...\")\n        model = get_peft_model(model, lora_config)\n        \n        # Move model to CPU\n        print(\"[CPU SETUP] Moving model to CPU...\")\n        model = model.to(device)\n        \n        print(\"[CPU SETUP] CPU configuration complete\")\n    \n    # Print trainable parameters\n    print(\"\\n[MODEL INFO] Trainable parameters:\")\n    model.print_trainable_parameters()\n    \n    # Print model architecture summary\n    print(\"\\n[MODEL INFO] Model architecture:\")\n    print(f\"  - Base model: {MODEL_NAME}\")\n    print(f\"  - Model type: {model.__class__.__name__}\")\n    print(f\"  - Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    print(f\"  - Training device: {device_type.upper()}\")\n    \n    # Check memory usage\n    process = psutil.Process(os.getpid())\n    memory_info = process.memory_info()\n    print(f\"\\n[SYSTEM INFO] Current memory usage: {memory_info.rss / 1024 / 1024:.2f} MB\")\n    \n    # Print success message\n    print(\"\\n\" + \"=\" * 50)\n    print(f\"MODEL SUCCESSFULLY LOADED ON {device_type.upper()}\")\n    print(\"=\" * 50 + \"\\n\")\n    \nexcept Exception as e:\n    print(f\"\\n[ERROR] Failed to load model: {e}\")\n    print(\"\\n[DEBUG INFO] Detailed error information:\")\n    import traceback\n    traceback.print_exc()\n    print(f\"\\n[DEBUG INFO] Device type: {device_type}\")\n    print(f\"[DEBUG INFO] Device: {device}\")\n    print(f\"[DEBUG INFO] Model name: {MODEL_NAME}\")\n    raise"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define hardware-specific training arguments\nprint(f\"\\n[TRAINING SETUP] Configuring training arguments for {device_type.upper()}...\")\n\n# Configure precision based on hardware\nif device_type == \"tpu\":\n    print(\"[TRAINING SETUP] Using TPU-specific training configuration\")\n    # TPU-specific settings\n    use_fp16 = False  # TPUs work better with bf16 than fp16\n    use_bf16 = True   # Use bfloat16 for TPU\n    tpu_metrics_debug = True  # Enable TPU metrics debugging\n    dataloader_workers = 8  # TPUs can handle more workers\n    \n    # TPU-specific environment variables\n    os.environ[\"XLA_USE_BF16\"] = \"1\"\n    print(\"[TRAINING SETUP] Using bfloat16 precision for TPU training\")\n    \n    # Adjust batch size and gradient accumulation for TPU\n    effective_batch_size = BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS\n    print(f\"[TRAINING SETUP] Effective batch size: {effective_batch_size}\")\n    \nelif device_type == \"gpu\":\n    print(\"[TRAINING SETUP] Using GPU-specific training configuration\")\n    # GPU-specific settings\n    use_fp16 = True   # Use fp16 for GPU\n    use_bf16 = False  # bf16 not optimal for most GPUs\n    tpu_metrics_debug = False\n    dataloader_workers = 2  # Fewer workers for GPU to avoid memory issues\n    \n    # Check if we have Ampere or newer GPU that supports TF32\n    if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8:\n        # Enable TF32 for faster training on Ampere+ GPUs\n        torch.backends.cuda.matmul.allow_tf32 = True\n        torch.backends.cudnn.allow_tf32 = True\n        print(\"[TRAINING SETUP] TF32 enabled for Ampere or newer GPU\")\n    \n    print(f\"[TRAINING SETUP] Using fp16 precision for GPU training\")\n    \nelse:  # CPU\n    print(\"[TRAINING SETUP] Using CPU-specific training configuration\")\n    # CPU-specific settings\n    use_fp16 = False  # No fp16 for CPU\n    use_bf16 = False  # No bf16 for CPU\n    tpu_metrics_debug = False\n    dataloader_workers = 0  # No multiprocessing for CPU to avoid overhead\n    \n    # Reduce batch size for CPU\n    if BATCH_SIZE > 1:\n        print(f\"[TRAINING SETUP] WARNING: Reducing batch size from {BATCH_SIZE} to 1 for CPU training\")\n        BATCH_SIZE = 1\n        GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS * 4  # Increase gradient accumulation to compensate\n    \n    print(f\"[TRAINING SETUP] Using float32 precision for CPU training\")\n\n# Define training arguments with hardware-specific optimizations\ntraining_args = TrainingArguments(\n    output_dir=\"./phi3_swift_results\",\n    num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    warmup_ratio=WARMUP_RATIO,\n    weight_decay=WEIGHT_DECAY,\n    learning_rate=LEARNING_RATE,\n    evaluation_strategy=\"steps\",\n    eval_steps=0.1,  # Evaluate every 10% of training\n    save_strategy=\"steps\",\n    save_steps=0.1,  # Save every 10% of training\n    load_best_model_at_end=True,\n    logging_dir=\"./logs\",\n    logging_steps=50,  # More frequent logging\n    save_total_limit=3,\n    fp16=use_fp16,\n    bf16=use_bf16,\n    report_to=\"none\",\n    remove_unused_columns=False,\n    push_to_hub=False,\n    disable_tqdm=False,\n    dataloader_num_workers=dataloader_workers,\n    dataloader_pin_memory=True,\n    group_by_length=True,  # Group similar length sequences for efficiency\n    tpu_metrics_debug=tpu_metrics_debug,  # Enable TPU metrics debugging if on TPU\n    # Additional optimizations\n    gradient_checkpointing=True,  # Enable gradient checkpointing to save memory\n    optim=\"adamw_torch\",  # Use PyTorch's AdamW implementation\n    ddp_find_unused_parameters=False,  # Optimize distributed training\n    torch_compile=False,  # Disable torch.compile for now as it can be unstable\n    seed=SEED,  # Ensure reproducibility\n)\n\nprint(f\"[TRAINING SETUP] Training arguments configured for {device_type.upper()}\")\nprint(f\"[TRAINING SETUP] Batch size: {BATCH_SIZE}, Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"[TRAINING SETUP] Learning rate: {LEARNING_RATE}, Epochs: {NUM_EPOCHS}\")\nprint(f\"[TRAINING SETUP] FP16: {use_fp16}, BF16: {use_bf16}\")\n\n# Define callbacks\nprint(\"[TRAINING SETUP] Setting up training callbacks...\")\n\n# Early stopping callback\nearly_stopping_callback = EarlyStoppingCallback(\n    early_stopping_patience=3,\n    early_stopping_threshold=0.01\n)\n\n# Resource monitoring callback\nresource_monitor = ResourceMonitorCallback(log_every=50)\n\n# Create data collator for language modeling\nprint(\"[TRAINING SETUP] Creating data collator for language modeling...\")\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False  # We're doing causal language modeling, not masked language modeling\n)\n\n# Create trainer with hardware-specific configuration\nprint(\"[TRAINING SETUP] Creating trainer...\")\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    callbacks=[early_stopping_callback, resource_monitor]\n)\n\n# Special handling for TPU\nif device_type == \"tpu\":\n    print(\"[TRAINING SETUP] Configuring TPU-specific training settings...\")\n    import torch_xla.core.xla_model as xm\n    import torch_xla.distributed.parallel_loader as pl\n    \n    # XLA-specific settings\n    xm.rendezvous(\"trainer_setup\")  # Synchronize TPU cores\n    print(\"[TRAINING SETUP] TPU cores synchronized\")\n\nprint(\"\\n[TRAINING SETUP] Training setup complete and ready to start\")\nprint(\"=\" * 50)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Function to monitor system resources during training\ndef monitor_resources():\n    process = psutil.Process(os.getpid())\n    memory_info = process.memory_info()\n    mem = psutil.virtual_memory()\n    cpu_percent = psutil.cpu_percent(interval=0.1)\n    \n    print(f\"\\n[SYSTEM RESOURCES]\")\n    print(f\"CPU Usage: {cpu_percent}%\")\n    print(f\"Process Memory: {memory_info.rss / 1024 / 1024:.2f} MB\")\n    print(f\"System Memory: {mem.percent}% used, {mem.available / 1024 / 1024:.2f} MB available\")\n    \n    # GPU-specific monitoring\n    if device_type == \"gpu\":\n        for i in range(torch.cuda.device_count()):\n            gpu_mem_alloc = torch.cuda.memory_allocated(i) / 1024 / 1024\n            gpu_mem_reserved = torch.cuda.memory_reserved(i) / 1024 / 1024\n            print(f\"GPU {i}: Allocated: {gpu_mem_alloc:.2f} MB | Reserved: {gpu_mem_reserved:.2f} MB\")\n    \n    # TPU-specific monitoring\n    if device_type == \"tpu\":\n        try:\n            import torch_xla.debug.metrics as met\n            print(\"TPU Memory Stats:\")\n            print(met.metrics_report())\n        except:\n            print(\"TPU metrics not available\")\n\n# Run training with more detailed monitoring\ntry:\n    print(\"\\n\" + \"=\" * 50)\n    print(\"STARTING TRAINING\")\n    print(\"=\" * 50)\n    \n    # Monitor resources before training\n    print(\"\\n[PRE-TRAINING] Resources before training:\")\n    monitor_resources()\n    \n    # Start training with a timeout\n    start_time = time.time()\n    \n    # Run training\n    print(\"\\n[TRAINING] Starting training process...\")\n    train_result = trainer.train()\n    \n    # Calculate training time\n    training_time = time.time() - start_time\n    hours, remainder = divmod(training_time, 3600)\n    minutes, seconds = divmod(remainder, 60)\n    \n    # Monitor resources after training\n    print(\"\\n[POST-TRAINING] Resources after training:\")\n    monitor_resources()\n    \n    # Print training results\n    print(\"\\n\" + \"=\" * 50)\n    print(\"TRAINING COMPLETED\")\n    print(\"=\" * 50)\n    print(f\"Training completed in {int(hours)}h {int(minutes)}m {seconds:.2f}s\")\n    print(f\"Training loss: {train_result.metrics['train_loss']:.4f}\")\n    \n    # Print all metrics\n    print(\"\\n[TRAINING METRICS]\")\n    for key, value in train_result.metrics.items():\n        print(f\"{key}: {value}\")\n    \n    # Save the model\n    print(\"\\n[SAVING MODEL] Saving model to ./phi3_swift_model\")\n    trainer.save_model(\"./phi3_swift_model\")\n    print(\"Model saved successfully\")\n    \n    # Save tokenizer\n    print(\"[SAVING MODEL] Saving tokenizer\")\n    tokenizer.save_pretrained(\"./phi3_swift_model\")\n    print(\"Tokenizer saved successfully\")\n    \n    # Clean up memory\n    print(\"\\n[CLEANUP] Cleaning up memory...\")\n    cleanup_memory()\n    \nexcept Exception as e:\n    print(f\"\\n[ERROR] Error during training: {e}\")\n    \n    # Print stack trace for debugging\n    print(\"\\n[DEBUG INFO] Detailed error information:\")\n    traceback.print_exc()\n    \n    # Monitor resources after error\n    print(\"\\n[DEBUG INFO] Resources after error:\")\n    monitor_resources()\n    \n    # Try to save checkpoint if possible\n    try:\n        print(\"\\n[RECOVERY] Attempting to save checkpoint after error...\")\n        trainer.save_model(\"./phi3_swift_model_error_recovery\")\n        print(\"Recovery checkpoint saved to ./phi3_swift_model_error_recovery\")\n    except:\n        print(\"Could not save recovery checkpoint\")\n    \n    raise"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test the model with Swift code examples\ntry:\n    print(\"\\n\" + \"=\" * 50)\n    print(\"TESTING THE MODEL\")\n    print(\"=\" * 50)\n    \n    # Function to generate responses for test examples\n    def generate_response(prompt):\n        print(f\"[TESTING] Generating response for prompt...\")\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n        \n        # Hardware-specific generation\n        with torch.no_grad():\n            if device_type == \"tpu\":\n                import torch_xla.core.xla_model as xm\n                outputs = model.generate(\n                    inputs.input_ids,\n                    max_new_tokens=200,\n                    temperature=0.7,\n                    top_p=0.9,\n                    do_sample=True,\n                    pad_token_id=tokenizer.pad_token_id,\n                    eos_token_id=tokenizer.eos_token_id\n                )\n                # Ensure the outputs are moved from TPU to CPU for decoding\n                outputs = xm.mesh_reduce('outputs', outputs, lambda x: x)\n            else:\n                outputs = model.generate(\n                    inputs.input_ids,\n                    max_new_tokens=200,\n                    temperature=0.7,\n                    top_p=0.9,\n                    do_sample=True,\n                    pad_token_id=tokenizer.pad_token_id,\n                    eos_token_id=tokenizer.eos_token_id\n                )\n        \n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        # Extract just the assistant's response\n        if \"<|assistant|>\" in response:\n            response = response.split(\"<|assistant|>\")[-1].strip()\n        return response\n    \n    # Test prompts for different Swift language tasks\n    test_prompts = [\n        # Explain Swift syntax\n        \"<|user|>\\nExplain the key features of Swift's optional unwrapping syntax:\\n\\n```swift\\nfunc processName(_ name: String?) {\\n    guard let unwrappedName = name else {\\n        print(\"No name provided\")\\n        return\\n    }\\n    print(\"Hello, \\(unwrappedName)!\")\\n}\\n```\\n<|assistant|>\",\n        \n        # Code completion\n        \"<|user|>\\nComplete this Swift function that calculates the factorial of a number:\\n\\n```swift\\nfunc factorial(_ n: Int) -> Int {\\n    // Add implementation here\\n}\\n```\\n<|assistant|>\",\n        \n        # Debugging help\n        \"<|user|>\\nWhat's wrong with this Swift code and how can I fix it?\\n\\n```swift\\nclass Person {\\n    var name: String\\n    var age: Int\\n    \\n    func greet() {\\n        print(\"Hello, my name is \\(name) and I am \\(age) years old.\")\\n    }\\n}\\n\\nlet person = Person()\\nperson.greet()\\n```\\n<|assistant|>\",\n        \n        # Swift best practices\n        \"<|user|>\\nExplain Swift best practices for error handling:\\n<|assistant|>\"\n    ]\n    \n    # Generate and print responses\n    for i, prompt in enumerate(test_prompts):\n        print(f\"\\n[TEST {i+1}]\\n{'-'*40}\")\n        print(f\"Prompt: {prompt.split('<|assistant|>')[0].replace('<|user|>', '')}\")\n        response = generate_response(prompt)\n        print(f\"\\nResponse:\\n{response}\\n\")\n        \n        # Clean up between tests\n        if device_type == \"tpu\":\n            import torch_xla.core.xla_model as xm\n            xm.mark_step()\n    \n    print(\"\\n[TESTING] Testing complete\")\nexcept Exception as e:\n    print(f\"\\n[ERROR] Error during testing: {e}\")\n    traceback.print_exc()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Final cleanup and summary\nprint(\"\\n\" + \"=\" * 50)\nprint(\"TRAINING SUMMARY\")\nprint(\"=\" * 50)\n\nprint(f\"Model: {MODEL_NAME}\")\nprint(f\"Training device: {device_type.upper()}\")\nprint(f\"Dataset: mvasiliniuc/iva-swift-codeint\")\nprint(f\"Training examples: {len(tokenized_train)}\")\nprint(f\"Validation examples: {len(tokenized_val)}\")\nprint(f\"Test examples: {len(tokenized_test)}\")\nprint(f\"Epochs: {NUM_EPOCHS}\")\nprint(f\"Batch size: {BATCH_SIZE}\")\nprint(f\"Learning rate: {LEARNING_RATE}\")\nprint(f\"LoRA rank: {LORA_R}\")\n\nprint(\"\\nModel saved to: ./phi3_swift_model\")\nprint(\"\\nTraining complete!\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}