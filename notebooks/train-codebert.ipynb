{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5e1db4c-8bd6-47a1-8462-4e44407f70d4",
   "metadata": {},
   "source": [
    "# CodeBERT for Swift Code Understanding\n",
    "\n",
    "In this notebook, we fine-tune the [CodeBERT](https://github.com/microsoft/CodeBERT) model on the [Swift Code Intelligence dataset](https://huggingface.co/datasets/mvasiliniuc/iva-swift-codeint). CodeBERT is a pre-trained model specifically designed for programming languages, much like how BERT was pre-trained for natural language text. Created by Microsoft Research, CodeBERT can understand both programming language and natural language, making it ideal for code-related tasks.\n",
    "\n",
    "We'll use the Swift code dataset to fine-tune the model for code understanding tasks. After training, we'll upload the model to Dropbox for easy access and distribution.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The process of fine-tuning CodeBERT involves:\n",
    "\n",
    "1. **ðŸ”§ Setup**: Install necessary libraries and prepare our environment\n",
    "2. **ðŸ“¥ Data Loading**: Load the Swift code dataset from Hugging Face\n",
    "3. **ðŸ§¹ Preprocessing**: Prepare the data for training by tokenizing the code samples\n",
    "4. **ðŸ§  Model Training**: Fine-tune CodeBERT on our prepared data\n",
    "5. **ðŸ“Š Evaluation**: Assess how well our model performs\n",
    "6. **ðŸ“¤ Export & Upload**: Save the model and upload it to Dropbox\n",
    "\n",
    "Let's start by installing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets evaluate torch scikit-learn tqdm dropbox requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c533ab-4907-4397-a4bd-11c8ac50fbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    RobertaForSequenceClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    set_seed\n",
    ")\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df57034d-bc42-472f-abfd-04a797218141",
   "metadata": {},
   "source": [
    "## Dataset and Model Configuration\n",
    "\n",
    "Let's define the model and dataset we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9627ab22-efd5-4270-9011-547028913250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model and dataset IDs\n",
    "MODEL_ID = \"microsoft/codebert-base\"\n",
    "DATASET_ID = \"mvasiliniuc/iva-swift-codeint\"\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce0e9e5-194c-40bd-bbc0-d11e917c3de3",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Now let's load the Swift code dataset and examine its structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba00863f-6db4-40ae-8a60-19abba7b244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset \n",
    "data = load_dataset(DATASET_ID, trust_remote_code=True)\n",
    "print(\"Dataset structure:\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at an example from the dataset\n",
    "if 'train' in data:\n",
    "    example = data['train'][0]\n",
    "else:\n",
    "    example = data[list(data.keys())[0]][0]\n",
    "    \n",
    "print(\"Example features:\")\n",
    "for key, value in example.items():\n",
    "    if isinstance(value, str) and len(value) > 100:\n",
    "        print(f\"{key}: {value[:100]}...\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenizer-section",
   "metadata": {},
   "source": [
    "## Loading the CodeBERT Tokenizer\n",
    "\n",
    "Now, let's load the CodeBERT tokenizer, which has been specially trained to handle code tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6500541f-d9a1-405d-89e3-9eaf645aad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CodeBERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"Tokenizer type: {tokenizer.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-preparation",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Since we're dealing with a code understanding task, we need to prepare our data appropriately. The dataset contains Swift code files, so we'll need to create labeled data for our task.\n",
    "\n",
    "For this demonstration, we'll create a binary classification task that determines whether the code is a Package.swift file (which is used for Swift package management) or not. This is just an example task - in a real application, you might have more complex classification targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classification dataset based on whether the file is a Package.swift file\n",
    "def add_labels(example):\n",
    "    # Label 1 if it's a Package.swift file, 0 otherwise\n",
    "    example['label'] = 1 if 'Package.swift' in example['path'] else 0\n",
    "    return example\n",
    "\n",
    "# Apply the labeling function\n",
    "labeled_data = data['train'].map(add_labels)\n",
    "\n",
    "# Check the distribution of labels using collections.Counter\n",
    "import collections\n",
    "all_labels = labeled_data['label']\n",
    "label_counter = collections.Counter(all_labels)\n",
    "print(\"Label distribution:\")\n",
    "for label, count in label_counter.items():\n",
    "    print(f\"Label {label}: {count} examples ({count/len(labeled_data)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-splitting",
   "metadata": {},
   "source": [
    "Now let's split our data into training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-splits",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset without stratification (to avoid ClassLabel errors)\n",
    "train_test_split = labeled_data.train_test_split(test_size=0.1, seed=42)\n",
    "train_data = train_test_split['train']\n",
    "val_data = train_test_split['test']\n",
    "\n",
    "# Verify label distribution after split\n",
    "train_label_counter = collections.Counter(train_data['label'])\n",
    "val_label_counter = collections.Counter(val_data['label'])\n",
    "\n",
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Training label distribution: {dict(train_label_counter)}\")\n",
    "print(f\"Validation set size: {len(val_data)}\")\n",
    "print(f\"Validation label distribution: {dict(val_label_counter)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7f09c-48f9-4196-83df-4b2fedc77ea9",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Now we need to tokenize our code samples. We'll use the CodeBERT tokenizer to convert the Swift code into token IDs that the model can understand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d122e349-613c-4353-9896-856f15daf4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the Swift code samples.\n",
    "    \n",
    "    Args:\n",
    "        examples: Batch of examples from the dataset\n",
    "        \n",
    "    Returns:\n",
    "        Tokenized examples\n",
    "    \"\"\"\n",
    "    # Tokenize the code content\n",
    "    return tokenizer(\n",
    "        examples[\"content\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,  # CodeBERT supports sequences up to 512 tokens\n",
    "        return_tensors=\"pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "process-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data\n",
    "tokenized_train_data = train_data.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[col for col in train_data.column_names if col != 'label']\n",
    ")\n",
    "\n",
    "tokenized_val_data = val_data.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[col for col in val_data.column_names if col != 'label']\n",
    ")\n",
    "\n",
    "print(\"Training data after tokenization:\")\n",
    "print(tokenized_train_data)\n",
    "print(\"\\nValidation data after tokenization:\")\n",
    "print(tokenized_val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-preparation",
   "metadata": {},
   "source": [
    "## Model Preparation\n",
    "\n",
    "Now that our data is ready, let's load the CodeBERT model and configure it for sequence classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CodeBERT model for sequence classification (2 classes)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID, num_labels=2)\n",
    "model.to(device)\n",
    "print(f\"Model type: {model.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-section",
   "metadata": {},
   "source": [
    "## Training Setup\n",
    "\n",
    "Now let's define our training arguments and evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute metrics during evaluation\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-training-args",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/codebert-swift\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_data,\n",
    "    eval_dataset=tokenized_val_data,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-execution",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Now let's train our CodeBERT model for Swift code classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting model training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-section",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "Let's evaluate our model on the validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prediction-section",
   "metadata": {},
   "source": [
    "## Testing the Model with Example Predictions\n",
    "\n",
    "Let's test our model on some sample Swift code files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some test examples\n",
    "test_examples = val_data.select(range(5))\n",
    "\n",
    "# Tokenize them\n",
    "tokenized_test_examples = tokenize_function({\"content\": test_examples[\"content\"]})\n",
    "\n",
    "# Move to device\n",
    "for key, val in tokenized_test_examples.items():\n",
    "    if isinstance(val, torch.Tensor):\n",
    "        tokenized_test_examples[key] = val.to(device)\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(**{k: v for k, v in tokenized_test_examples.items() if k != \"label\"})\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    predicted_labels = torch.argmax(predictions, dim=-1).cpu().numpy()\n",
    "\n",
    "# Print results\n",
    "for i, (pred, true) in enumerate(zip(predicted_labels, test_examples[\"label\"])):\n",
    "    is_package_swift = \"Yes\" if pred == 1 else \"No\"\n",
    "    true_is_package_swift = \"Yes\" if true == 1 else \"No\"\n",
    "    print(f\"File path: {test_examples['path'][i]}\")\n",
    "    print(f\"Prediction: Is Package.swift? {is_package_swift} (Confidence: {predictions[i][pred].item():.4f})\")\n",
    "    print(f\"True label: Is Package.swift? {true_is_package_swift}\")\n",
    "    print(f\"First few lines: {test_examples['content'][i][:100]}...\")\n",
    "    print(\"---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-section",
   "metadata": {},
   "source": [
    "## Saving the Model\n",
    "\n",
    "Now let's save the model and tokenizer for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for the model\n",
    "model_save_dir = \"./codebert-swift-model\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(model_save_dir)\n",
    "tokenizer.save_pretrained(model_save_dir)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {model_save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dropbox-section",
   "metadata": {},
   "source": [
    "## Uploading to Dropbox\n",
    "\n",
    "Now let's upload our trained model to Dropbox for easy access and distribution. We'll use the same approach as in the Groq downloader notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload-prepare",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import dropbox\n",
    "from dropbox.files import WriteMode\n",
    "from dropbox.exceptions import ApiError, AuthError\n",
    "\n",
    "# First, let's zip the model directory\n",
    "def zip_directory(directory, output_path):\n",
    "    \"\"\"Compress a directory into a zip file.\"\"\"\n",
    "    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, _, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                zipf.write(file_path, os.path.relpath(file_path, os.path.dirname(directory)))\n",
    "        print(f\"Created zip file: {output_path}\")\n",
    "\n",
    "# Zip the model directory\n",
    "model_zip_path = \"./codebert-swift-model.zip\"\n",
    "zip_directory(model_save_dir, model_zip_path)\n",
    "\n",
    "# Check the size of the zip file\n",
    "zip_size_mb = os.path.getsize(model_zip_path) / (1024 * 1024)\n",
    "print(f\"Zip file size: {zip_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dropbox-credentials",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropbox API credentials\n",
    "APP_KEY = \"2bi422xpd3xd962\"\n",
    "APP_SECRET = \"j3yx0b41qdvfu86\"\n",
    "REFRESH_TOKEN = \"RvyL03RE5qAAAAAAAAAAAVMVebvE7jDx8Okd0ploMzr85c6txvCRXpJAt30mxrKF\"\n",
    "\n",
    "# Initialize Dropbox client\n",
    "def get_dropbox_client():\n",
    "    \"\"\"Initialize and authenticate Dropbox client using refresh token.\"\"\"\n",
    "    try:\n",
    "        dbx = dropbox.Dropbox(\n",
    "            app_key=APP_KEY,\n",
    "            app_secret=APP_SECRET,\n",
    "            oauth2_refresh_token=REFRESH_TOKEN\n",
    "        )\n",
    "        # Check that the access token is valid\n",
    "        dbx.users_get_current_account()\n",
    "        return dbx\n",
    "    except AuthError as e:\n",
    "        print(f\"ERROR: Invalid credentials. {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload-to-dropbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the file to Dropbox\n",
    "def upload_file_to_dropbox(file_path, dropbox_path):\n",
    "    \"\"\"Upload a file to Dropbox.\"\"\"\n",
    "    dbx = get_dropbox_client()\n",
    "    if not dbx:\n",
    "        return False\n",
    "        \n",
    "    with open(file_path, 'rb') as f:\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        chunk_size = 4 * 1024 * 1024  # 4MB chunks\n",
    "        \n",
    "        if file_size <= chunk_size:\n",
    "            # Small file, upload in one go\n",
    "            print(f\"Uploading {file_path} to Dropbox as {dropbox_path}...\")\n",
    "            try:\n",
    "                dbx.files_upload(f.read(), dropbox_path, mode=WriteMode('overwrite'))\n",
    "                print(\"Upload complete!\")\n",
    "                return True\n",
    "            except ApiError as e:\n",
    "                print(f\"ERROR: Dropbox API error - {e}\")\n",
    "                return False\n",
    "        else:\n",
    "            # Large file, use chunked upload\n",
    "            print(f\"Uploading {file_path} to Dropbox as {dropbox_path} in chunks...\")\n",
    "            upload_session_start_result = dbx.files_upload_session_start(f.read(chunk_size))\n",
    "            cursor = dropbox.files.UploadSessionCursor(\n",
    "                session_id=upload_session_start_result.session_id,\n",
    "                offset=f.tell()\n",
    "            )\n",
    "            commit = dropbox.files.CommitInfo(path=dropbox_path, mode=WriteMode('overwrite'))\n",
    "            \n",
    "            # Upload the file in chunks\n",
    "            uploaded = f.tell()\n",
    "            with tqdm(total=file_size, desc=\"Uploading\", unit=\"B\", unit_scale=True) as pbar:\n",
    "                pbar.update(uploaded)\n",
    "                \n",
    "                while uploaded < file_size:\n",
    "                    if (file_size - uploaded) <= chunk_size:\n",
    "                        # Last chunk\n",
    "                        data = f.read(chunk_size)\n",
    "                        dbx.files_upload_session_finish(\n",
    "                            data, cursor, commit\n",
    "                        )\n",
    "                        uploaded += len(data)\n",
    "                        pbar.update(len(data))\n",
    "                    else:\n",
    "                        # More chunks to upload\n",
    "                        data = f.read(chunk_size)\n",
    "                        dbx.files_upload_session_append_v2(\n",
    "                            data, cursor\n",
    "                        )\n",
    "                        uploaded += len(data)\n",
    "                        cursor.offset = uploaded\n",
    "                        pbar.update(len(data))\n",
    "                        \n",
    "            print(\"Chunked upload complete!\")\n",
    "            return True\n",
    "\n",
    "# Upload the model zip to Dropbox\n",
    "dropbox_path = \"/codebert-swift-model/codebert-swift-model.zip\"\n",
    "success = upload_file_to_dropbox(model_zip_path, dropbox_path)\n",
    "\n",
    "if success:\n",
    "    print(f\"Successfully uploaded model to Dropbox at {dropbox_path}\")\n",
    "else:\n",
    "    print(\"Failed to upload model to Dropbox.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generate-link",
   "metadata": {},
   "source": [
    "## Creating a Shareable Link\n",
    "\n",
    "Finally, let's create a shareable link for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-link",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a shared link for the file\n",
    "def create_shared_link(dropbox_path):\n",
    "    \"\"\"Create a shared link for a file in Dropbox.\"\"\"\n",
    "    dbx = get_dropbox_client()\n",
    "    if not dbx:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        shared_link = dbx.sharing_create_shared_link_with_settings(dropbox_path)\n",
    "        return shared_link.url\n",
    "    except ApiError as e:\n",
    "        # If the file already has a shared link, the API will return an error\n",
    "        if isinstance(e.error, dropbox.sharing.CreateSharedLinkWithSettingsError) and \\\n",
    "           e.error.is_shared_link_already_exists():\n",
    "            # Get existing links\n",
    "            links = dbx.sharing_list_shared_links(dropbox_path).links\n",
    "            if links:\n",
    "                return links[0].url\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return None\n",
    "\n",
    "# Create a shared link\n",
    "shared_link = create_shared_link(dropbox_path)\n",
    "\n",
    "if shared_link:\n",
    "    # Convert to direct download link\n",
    "    download_link = shared_link.replace(\"www.dropbox.com\", \"dl.dropboxusercontent.com\").replace(\"?dl=0\", \"\")\n",
    "    print(f\"Download link: {download_link}\")\n",
    "else:\n",
    "    print(\"Failed to create shared link.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've successfully:\n",
    "\n",
    "1. Loaded and prepared the Swift code dataset for training\n",
    "2. Fine-tuned the CodeBERT model on this dataset\n",
    "3. Evaluated the model's performance\n",
    "4. Saved and uploaded the model to Dropbox for easy access\n",
    "\n",
    "This fine-tuned CodeBERT model can now be used for various Swift code understanding tasks, such as code search, code classification, or as a feature extractor for more complex code intelligence tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}