{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "tpu-v3-8",
   "dataSources": [],
   "dockerImageVersionId": 31013,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }}
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# CodeBERT for Swift Code Understanding\n\nIn this notebook, we fine-tune the [CodeBERT](https://github.com/microsoft/CodeBERT) model on the [Swift Code Intelligence dataset](https://huggingface.co/datasets/mvasiliniuc/iva-swift-codeint). CodeBERT is a pre-trained model designed for programming languages, ideal for code-related tasks.\n\nWe'll use the Swift code dataset for a binary classification task (detecting `Package.swift` files). After training, we'll upload the model to Dropbox.\n\n## Overview\n\n1. **ðŸ”§ Setup**: Install libraries and configure TPU\n2. **ðŸ“¥ Data Loading**: Load the dataset\n3. **ðŸ§¹ Preprocessing**: Tokenize and label data\n4. **ðŸ§  Model Training**: Fine-tune CodeBERT\n5. **ðŸ“Š Evaluation**: Assess performance\n6. **ðŸ“¤ Export & Upload**: Save and upload to Dropbox",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!pip install transformers datasets evaluate torch scikit-learn tqdm dropbox requests torch_xla accelerate -U\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import os\nimport json\nimport time\nimport torch\nimport random\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom datasets import load_dataset\nfrom sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    Trainer,\n    TrainingArguments,\n    set_seed,\n    DataCollatorWithPadding,\n    EarlyStoppingCallback\n)\n\n# TPU-specific imports\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\nfrom torch_xla.distributed.parallel_loader import ParallelLoader\n\n# Set a seed for reproducibility\nset_seed(42)\n\n# Initialize TPU\ntry:\n    # Check if TPU is available\n    import torch_xla.utils.utils as xu\n    tpu_available = xu.getenv_as('XRT_TPU_CONFIG', '') != ''\n    \n    if tpu_available:\n        device = xm.xla_device()\n        print(f\"Using TPU device: {device}\")\n        \n        # Set up TPU-specific logging\n        import logging\n        logging.basicConfig(\n            format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n            datefmt=\"%m/%d/%Y %H:%M:%S\",\n            level=logging.INFO\n        )\n        \n        # Configure XLA for TPU\n        os.environ['XLA_USE_BF16'] = '1'  # Enable bfloat16 for better performance\n        print(\"TPU configured with bfloat16 support\")\n    else:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        print(f\"TPU not available, using device: {device}\")\nexcept Exception as e:\n    print(f\"Error initializing TPU: {e}\")\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Falling back to device: {device}\")\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Dataset and Model Configuration",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Set model and dataset IDs\nMODEL_ID = \"microsoft/codebert-base\"\nDATASET_ID = \"mvasiliniuc/iva-swift-codeint\"\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Data Loading",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load the dataset with retry logic
def load_dataset_with_retry(dataset_id, max_retries=3, retry_delay=5):\n    for attempt in range(max_retries):\n        try:\n            print(f\"Loading dataset (attempt {attempt+1}/{max_retries})...\")\n            dataset = load_dataset(dataset_id, trust_remote_code=True)\n            return dataset\n        except Exception as e:\n            print(f\"Error loading dataset (attempt {attempt+1}): {e}\")\n            if attempt < max_retries - 1:\n                print(f\"Retrying in {retry_delay} seconds...\")\n                time.sleep(retry_delay)\n            else:\n                raise ValueError(f\"Failed to load dataset after {max_retries} attempts: {e}\")\n\ntry:\n    data = load_dataset_with_retry(DATASET_ID)\n    \n    # Verify dataset structure and column names\n    expected_columns = ['content', 'path']\n    if 'train' in data:\n        dataset_columns = data['train'].column_names\n    else:\n        dataset_columns = data[list(data.keys())[0]].column_names\n    \n    missing_columns = [col for col in expected_columns if col not in dataset_columns]\n    if missing_columns:\n        raise ValueError(f\"Dataset missing required columns: {missing_columns}\")\n    \n    # Print dataset information\n    print(\"Dataset structure:\")\n    print(data)\n    print(f\"Dataset columns: {dataset_columns}\")\n    \n    # Verify dataset size\n    for split, dataset in data.items():\n        print(f\"Split '{split}' size: {len(dataset)} examples\")\n        if len(dataset) == 0:\n            raise ValueError(f\"Empty dataset split: {split}\")\n\nexcept Exception as e:\n    print(f\"Error loading or verifying dataset: {e}\")\n    raise\n\n# Load tokenizer before analyzing sequence lengths\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n    print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n    \n    # Analyze sequence lengths to set max_length\n    print(\"Analyzing sequence lengths...\")\n    data_sample = data['train'][:1000] if 'train' in data else data[list(data.keys())[0]][:1000]\n    \n    # Use batched tokenization to reduce memory usage\n    def get_lengths(examples, batch_size=32):\n        lengths = []\n        for i in range(0, len(examples), batch_size):\n            batch = examples[i:i+batch_size]\n            batch_lengths = [len(tokenizer.encode(sample['content'], truncation=True, max_length=512)) \n                            for sample in batch]\n            lengths.extend(batch_lengths)\n        return lengths\n    \n    lengths = get_lengths(data_sample)\n    max_length = min(512, int(np.percentile(lengths, 95)))\n    print(f\"Determined max_length: {max_length}\")\n    print(f\"Length statistics: min={min(lengths)}, max={max(lengths)}, mean={np.mean(lengths):.1f}, median={np.median(lengths)}\")\nexcept Exception as e:\n    print(f\"Error during tokenization or sequence length analysis: {e}\")\n    raise\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Example from the dataset\ntry:\n    example = data['train'][0] if 'train' in data else data[list(data.keys())[0]][0]\n    print(\"Example features:\")\n    for key, value in example.items():\n        if isinstance(value, str) and len(value) > 100:\n            print(f\"{key}: {value[:100]}...\")\n        else:\n            print(f\"{key}: {value}\")\nexcept Exception as e:\n    print(f\"Error accessing example: {e}\")\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Loading the CodeBERT Tokenizer",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Additional tokenizer information\ntry:\n    print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n    print(f\"Tokenizer pad token: {tokenizer.pad_token}\")\n    print(f\"Tokenizer special tokens: {tokenizer.all_special_tokens}\")\n    \n    # Test tokenization on a sample\n    sample_text = \"func main() { print(\\\"Hello, world!\\\") }\"\n    encoded = tokenizer(sample_text, return_tensors=\"pt\")\n    print(f\"Sample tokenization output shape: {encoded.input_ids.shape}\")\n    print(f\"Sample tokenization: {encoded.input_ids[0][:10]}...\")\nexcept Exception as e:\n    print(f\"Error analyzing tokenizer: {e}\")\n    raise\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Data Preparation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Add labels for Package.swift detection\ndef add_labels(example):\n    example['label'] = 1 if 'Package.swift' in example['path'] else 0\n    return example\n\ntry:\n    # Apply labeling to all splits\n    labeled_data = {}\n    for split in data.keys():\n        labeled_data[split] = data[split].map(add_labels)\n    \n    # Check label distribution in each split\n    import collections\n    from sklearn.utils.class_weight import compute_class_weight\n    \n    for split, dataset in labeled_data.items():\n        all_labels = dataset['label']\n        label_counter = collections.Counter(all_labels)\n        print(f\"\\nLabel distribution in {split} split:\")\n        for label, count in label_counter.items():\n            print(f\"Label {label}: {count} examples ({count/len(dataset)*100:.2f}%)\")\n        \n        # Check for severe imbalance (if one class is less than 10% of the data)\n        min_class_pct = min([count/len(dataset)*100 for count in label_counter.values()])\n        if min_class_pct < 10:\n            print(f\"WARNING: Severe class imbalance detected in {split} split! Minority class is only {min_class_pct:.2f}%\")\n    \n    # Handle imbalance with class weights for training data\n    train_labels = labeled_data['train']['label']\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n    print(f\"\\nComputed class weights: {dict(enumerate(class_weights))}\")\n    \n    # If severe imbalance, consider additional techniques\n    min_class_pct = min([count/len(labeled_data['train'])*100 for count in collections.Counter(train_labels).values()])\n    if min_class_pct < 5:\n        print(\"\\nSevere imbalance detected. Implementing additional balancing techniques:\")\n        # Option 1: Oversampling minority class\n        from sklearn.utils import resample\n        \n        # Get indices of majority and minority classes\n        train_data = labeled_data['train']\n        minority_label = 1 if collections.Counter(train_labels)[1] < collections.Counter(train_labels)[0] else 0\n        majority_label = 1 - minority_label\n        \n        minority_indices = [i for i, label in enumerate(train_labels) if label == minority_label]\n        majority_indices = [i for i, label in enumerate(train_labels) if label == majority_label]\n        \n        # Calculate how many samples to generate\n        num_to_oversample = min(len(majority_indices) - len(minority_indices), len(minority_indices) * 2)\n        print(f\"Oversampling minority class (label {minority_label}) by {num_to_oversample} examples\")\n        \n        # Will use class weights in combination with oversampling\n        print(\"Will use class weights in combination with balanced sampling during training\")\nexcept Exception as e:\n    print(f\"Error preparing data: {e}\")\n    raise\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Prepare train/validation/test splits with stratification\ntry:\n    # Check if we already have predefined splits\n    if 'validation' in labeled_data and 'test' in labeled_data:\n        print(\"Using predefined dataset splits\")\n        train_data = labeled_data['train']\n        val_data = labeled_data['validation']\n        test_data = labeled_data['test']\n    elif 'test' in labeled_data:\n        print(\"Dataset has test split but no validation split. Creating validation split from train data.\")\n        # Create validation split from train data\n        train_val_split = labeled_data['train'].train_test_split(\n            test_size=0.1, \n            stratify=labeled_data['train']['label'], \n            seed=42\n        )\n        train_data = train_val_split['train']\n        val_data = train_val_split['test']\n        test_data = labeled_data['test']\n    else:\n        print(\"Creating train/validation/test splits from single dataset\")\n        # First split off test set (10%)\n        train_test_split = labeled_data['train'].train_test_split(\n            test_size=0.1, \n            stratify=labeled_data['train']['label'], \n            seed=42\n        )\n        # Then split remaining data into train/val (90/10 split of remaining data)\n        train_val_split = train_test_split['train'].train_test_split(\n            test_size=0.1, \n            stratify=train_test_split['train']['label'], \n            seed=42\n        )\n        train_data = train_val_split['train']\n        val_data = train_val_split['test']\n        test_data = train_test_split['test']\n    \n    # Verify label distribution and sizes\n    train_label_counter = collections.Counter(train_data['label'])\n    val_label_counter = collections.Counter(val_data['label'])\n    test_label_counter = collections.Counter(test_data['label']) if test_data else None\n    \n    print(f\"\\nTraining set size: {len(train_data)}\")\n    print(f\"Training label distribution: {dict(train_label_counter)}\")\n    print(f\"Training label percentages: {[(label, count/len(train_data)*100) for label, count in train_label_counter.items()]}\")\n    \n    print(f\"\\nValidation set size: {len(val_data)}\")\n    print(f\"Validation label distribution: {dict(val_label_counter)}\")\n    print(f\"Validation label percentages: {[(label, count/len(val_data)*100) for label, count in val_label_counter.items()]}\")\n    \n    if test_data:\n        print(f\"\\nTest set size: {len(test_data)}\")\n        print(f\"Test label distribution: {dict(test_label_counter)}\")\n        print(f\"Test label percentages: {[(label, count/len(test_data)*100) for label, count in test_label_counter.items()]}\")\n    \n    # Verify that stratification worked properly\n    train_minority_pct = min([count/len(train_data)*100 for count in train_label_counter.values()])\n    val_minority_pct = min([count/len(val_data)*100 for count in val_label_counter.values()])\n    \n    if abs(train_minority_pct - val_minority_pct) > 5:\n        print(\"\\nWARNING: Significant difference in class distribution between train and validation sets!\")\n        print(f\"Train minority class: {train_minority_pct:.2f}%, Validation minority class: {val_minority_pct:.2f}%\")\n        print(\"This may indicate issues with stratification.\")\nexcept Exception as e:\n    print(f\"Error preparing dataset splits: {e}\")\n    raise\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Tokenization",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def tokenize_function(examples):\n    # Use dynamic padding with a data collator later instead of padding here\n    # This significantly reduces memory usage during preprocessing\n    return tokenizer(\n        examples[\"content\"],\n        padding=False,  # No padding during preprocessing - will use DataCollator\n        truncation=True,\n        max_length=max_length,\n    )\n\ntry:\n    # Process datasets with memory-efficient batching\n    batch_size = 32  # Smaller batch size to reduce memory usage\n    \n    print(\"Tokenizing training data...\")\n    tokenized_train_data = train_data.map(\n        tokenize_function,\n        batched=True,\n        batch_size=batch_size,\n        remove_columns=[col for col in train_data.column_names if col != 'label'],\n        desc=\"Tokenizing training data\"\n    )\n    \n    print(\"Tokenizing validation data...\")\n    tokenized_val_data = val_data.map(\n        tokenize_function,\n        batched=True,\n        batch_size=batch_size,\n        remove_columns=[col for col in val_data.column_names if col != 'label'],\n        desc=\"Tokenizing validation data\"\n    )\n    \n    # Also tokenize test data if available\n    if 'test_data' in locals() and test_data is not None:\n        print(\"Tokenizing test data...\")\n        tokenized_test_data = test_data.map(\n            tokenize_function,\n            batched=True,\n            batch_size=batch_size,\n            remove_columns=[col for col in test_data.column_names if col != 'label'],\n            desc=\"Tokenizing test data\"\n        )\n    \n    # Verify tokenized data structure\n    print(\"\\nTraining data after tokenization:\")\n    print(tokenized_train_data)\n    print(f\"Number of training examples: {len(tokenized_train_data)}\")\n    print(f\"Features: {tokenized_train_data.column_names}\")\n    \n    print(\"\\nValidation data after tokenization:\")\n    print(tokenized_val_data)\n    print(f\"Number of validation examples: {len(tokenized_val_data)}\")\n    \n    if 'tokenized_test_data' in locals():\n        print(\"\\nTest data after tokenization:\")\n        print(tokenized_test_data)\n        print(f\"Number of test examples: {len(tokenized_test_data)}\")\n    \n    # Check for any truncated examples\n    def count_truncated(dataset):\n        # Check if any examples were truncated to max_length\n        sample_size = min(1000, len(dataset))\n        sample = dataset.select(range(sample_size))\n        original_lengths = [len(tokenizer.encode(train_data[i]['content'])) for i in range(sample_size)]\n        truncated = sum(1 for length in original_lengths if length > max_length)\n        return truncated, sample_size, truncated/sample_size*100 if sample_size > 0 else 0\n    \n    train_truncated, train_sample_size, train_truncated_pct = count_truncated(train_data)\n    print(f\"\\nTruncated training examples: {train_truncated}/{train_sample_size} ({train_truncated_pct:.2f}%)\")\n    \n    if train_truncated_pct > 20:\n        print(\"WARNING: A significant portion of training examples were truncated.\")\n        print(\"Consider increasing max_length if model performance is affected.\")\nexcept Exception as e:\n    print(f\"Error tokenizing data: {e}\")\n    raise\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Model Preparation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load the CodeBERT model with TPU optimizations\ntry:\n    # Load model with proper configuration for TPU\n    model = AutoModelForSequenceClassification.from_pretrained(\n        MODEL_ID, \n        num_labels=2,\n        return_dict=True\n    )\n    \n    # Print model information\n    print(f\"Model type: {model.__class__.__name__}\")\n    print(f\"Model size: {sum(p.numel() for p in model.parameters())/1000000:.2f}M parameters\")\n    \n    # Apply TPU-specific optimizations\n    if tpu_available:\n        # Move model to TPU\n        model.to(device)\n        print(\"Model moved to TPU device\")\n        \n        # Enable gradient checkpointing to save memory if available\n        if hasattr(model, 'gradient_checkpointing_enable'):\n            model.gradient_checkpointing_enable()\n            print(\"Gradient checkpointing enabled to save memory\")\n    else:\n        # Move model to available device (GPU/CPU)\n        model.to(device)\n        print(f\"Model moved to {device}\")\n    \n    # Verify model device placement\n    print(f\"Model is on device: {next(model.parameters()).device}\")\nexcept Exception as e:\n    print(f\"Error loading model: {e}\")\n    raise\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Training Setup",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def compute_metrics(eval_preds):\n    \"\"\"Compute comprehensive evaluation metrics for model performance.\"\"\"\n    try:\n        logits, labels = eval_preds\n        predictions = np.argmax(logits, axis=-1)\n        \n        # Basic metrics\n        accuracy = accuracy_score(labels, predictions)\n        \n        # Detailed metrics\n        precision, recall, f1, support = precision_recall_fscore_support(\n            labels, predictions, average=None, labels=[0, 1]\n        )\n        \n        # Weighted metrics (accounts for class imbalance)\n        weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n            labels, predictions, average='weighted'\n        )\n        \n        # Create detailed metrics dictionary\n        metrics = {\n            'accuracy': accuracy,\n            'weighted_f1': weighted_f1,\n            'weighted_precision': weighted_precision,\n            'weighted_recall': weighted_recall,\n            'f1_class0': f1[0],\n            'f1_class1': f1[1],\n            'precision_class0': precision[0],\n            'precision_class1': precision[1],\n            'recall_class0': recall[0],\n            'recall_class1': recall[1],\n            'support_class0': int(support[0]),\n            'support_class1': int(support[1])\n        }\n        \n        # Add confusion matrix data\n        from sklearn.metrics import confusion_matrix\n        cm = confusion_matrix(labels, predictions)\n        if cm.shape == (2, 2):\n            tn, fp, fn, tp = cm.ravel()\n            metrics.update({\n                'true_negatives': int(tn),\n                'false_positives': int(fp),\n                'false_negatives': int(fn),\n                'true_positives': int(tp)\n            })\n        \n        return metrics\n    except Exception as e:\n        print(f\"Error computing metrics: {e}\")\n        # Return basic metrics if detailed calculation fails\n        return {\n            'accuracy': accuracy_score(labels, predictions),\n            'f1': f1_score(labels, predictions, average='weighted')\n        }\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Set up training arguments with TPU-specific configurations\ntry:\n    # Convert class weights to a list for use in training arguments\n    class_weight_list = list(class_weights)\n    \n    # Define training arguments with TPU optimizations\n    training_args = TrainingArguments(\n        output_dir=\"./results/codebert-swift\",\n        evaluation_strategy=\"epoch\",  # Fixed deprecated parameter\n        save_strategy=\"epoch\",\n        learning_rate=5e-5,\n        per_device_train_batch_size=32 if tpu_available else 16,  # Larger batch size for TPU\n        per_device_eval_batch_size=64 if tpu_available else 32,   # Larger eval batch size\n        num_train_epochs=3,\n        weight_decay=0.01,\n        logging_dir=\"./logs\",\n        logging_steps=100,  # More frequent logging\n        save_total_limit=2,  # Limit number of checkpoints to save disk space\n        load_best_model_at_end=True,\n        metric_for_best_model=\"weighted_f1\",  # Use weighted F1 for imbalanced data\n        greater_is_better=True,\n        push_to_hub=False,\n        # TPU-specific settings\n        dataloader_drop_last=True,  # Helps with TPU efficiency\n        bf16=tpu_available,  # Use bfloat16 on TPU for better performance\n        tf32=False,  # Not needed with bfloat16\n        dataloader_num_workers=4 if tpu_available else 2,  # Parallel data loading\n        gradient_accumulation_steps=4 if tpu_available else 1,  # Accumulate gradients for larger effective batch size\n        # Early stopping\n        early_stopping_patience=3,\n        # Checkpointing\n        save_steps=500,\n        # Warmup\n        warmup_steps=500,\n        # Reporting\n        report_to=\"none\",  # Disable wandb/tensorboard reporting\n    )\n    \n    print(f\"Training configuration:\\n{training_args}\")\n    \n    # Create data collator for dynamic padding (memory efficient)\n    data_collator = DataCollatorWithPadding(\n        tokenizer=tokenizer,\n        padding=\"longest\",  # Dynamic padding\n        max_length=max_length,\n        pad_to_multiple_of=8 if tpu_available else None  # TPU optimization\n    )\n    \n    # Create trainer with early stopping\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_train_data,\n        eval_dataset=tokenized_val_data,\n        compute_metrics=compute_metrics,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n    )\n    \n    # Set class weights for handling imbalance\n    if hasattr(trainer, 'class_weights'):\n        trainer.class_weights = torch.tensor(class_weight_list).to(device)\n        print(f\"Set class weights in trainer: {class_weight_list}\")\n    else:\n        print(\"Class weights will be handled through loss function\")\n        # Define a weighted loss function\n        class_weights_tensor = torch.tensor(class_weight_list).to(device)\n        def weighted_loss(outputs, labels):\n            loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n            return loss_fct(outputs.view(-1, 2), labels.view(-1))\n        trainer.compute_loss = weighted_loss\n        print(\"Custom weighted loss function configured\")\n    \n    # Verify trainer configuration\n    print(f\"Trainer prepared with:\\n- Model: {model.__class__.__name__}\")\n    print(f\"- Training examples: {len(tokenized_train_data)}\")\n    print(f\"- Validation examples: {len(tokenized_val_data)}\")\n    print(f\"- Batch size: {training_args.per_device_train_batch_size} * {training_args.gradient_accumulation_steps} (effective) per device\")\n    print(f\"- Epochs: {training_args.num_train_epochs}\")\n    print(f\"- Learning rate: {training_args.learning_rate}\")\n    print(f\"- Device: {device}\")\nexcept Exception as e:\n    print(f\"Error setting up trainer: {e}\")\n    raise\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Training the Model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Training with TPU-specific monitoring and error handling\ntry:\n    print(\"Starting model training...\")\n    \n    # Set up TPU-specific monitoring if available\n    if tpu_available:\n        import time\n        from tqdm.auto import tqdm\n        \n        # Function to log TPU memory usage\n        def log_tpu_memory_usage():\n            try:\n                import torch_xla.debug.metrics as met\n                memory_usage = met.metrics_report()\n                print(\"\\nTPU Memory Usage:\")\n                for line in memory_usage.split('\\n'):\n                    if 'Memory' in line:\n                        print(line.strip())\n            except Exception as e:\n                print(f\"Could not log TPU memory: {e}\")\n        \n        # Log initial TPU state\n        print(\"\\nInitial TPU state:\")\n        log_tpu_memory_usage()\n        \n        # Set up checkpoint recovery mechanism\n        checkpoint_path = os.path.join(training_args.output_dir, \"checkpoint-recovery\")\n        os.makedirs(checkpoint_path, exist_ok=True)\n        print(f\"\\nCheckpoint recovery directory created at {checkpoint_path}\")\n        \n        # Start training with periodic TPU monitoring\n        start_time = time.time()\n        train_result = trainer.train()\n        \n        # Log final TPU state\n        print(\"\\nFinal TPU state after training:\")\n        log_tpu_memory_usage()\n    else:\n        # Regular training for non-TPU devices\n        train_result = trainer.train()\n    \n    # Print training results\n    print(f\"\\nTraining completed in {(time.time() - start_time)/60:.2f} minutes\")\n    print(f\"Training metrics: {train_result.metrics}\")\n    \n    # Save the final model\n    trainer.save_model()\n    print(\"Final model saved\")\n    \n    # Save training state\n    trainer.state.save_to_json(os.path.join(training_args.output_dir, \"trainer_state.json\"))\n    print(\"Training state saved\")\nexcept Exception as e:\n    print(f\"\\nERROR during training: {e}\")\n    \n    # Try to save checkpoint if possible\n    try:\n        print(\"Attempting to save emergency checkpoint...\")\n        emergency_dir = \"./emergency-checkpoint\"\n        os.makedirs(emergency_dir, exist_ok=True)\n        if 'trainer' in locals():\n            trainer.save_model(emergency_dir)\n            print(f\"Emergency checkpoint saved to {emergency_dir}\")\n    except Exception as recovery_error:\n        print(f\"Could not save emergency checkpoint: {recovery_error}\")\n    \n    # Re-raise the original error\n    raise\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluating the Model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Comprehensive evaluation with improved sampling\ntry:\n    print(\"\\nRunning comprehensive evaluation...\")\n    \n    # Evaluate on validation set\n    val_results = trainer.evaluate(eval_dataset=tokenized_val_data)\n    print(f\"\\nValidation results:\\n{val_results}\")\n    \n    # Evaluate on test set if available\n    if 'tokenized_test_data' in locals():\n        test_results = trainer.evaluate(eval_dataset=tokenized_test_data, metric_key_prefix=\"test\")\n        print(f\"\\nTest set results:\\n{test_results}\")\n    \n    # Evaluate on class-balanced sample for better representation\n    try:\n        print(\"\\nEvaluating on class-balanced sample...\")\n        # Create a balanced sample from validation set\n        val_labels = tokenized_val_data['label']\n        class_0_indices = [i for i, label in enumerate(val_labels) if label == 0]\n        class_1_indices = [i for i, label in enumerate(val_labels) if label == 1]\n        \n        # Sample equal numbers from each class (up to 100 per class)\n        sample_size = min(100, min(len(class_0_indices), len(class_1_indices)))\n        import random\n        random.seed(42)  # For reproducibility\n        \n        sampled_class_0 = random.sample(class_0_indices, sample_size)\n        sampled_class_1 = random.sample(class_1_indices, sample_size)\n        balanced_indices = sampled_class_0 + sampled_class_1\n        \n        # Create balanced dataset\n        balanced_eval_dataset = tokenized_val_data.select(balanced_indices)\n        print(f\"Created balanced evaluation sample with {len(balanced_eval_dataset)} examples\")\n        \n        # Evaluate on balanced sample\n        balanced_results = trainer.evaluate(eval_dataset=balanced_eval_dataset, metric_key_prefix=\"balanced\")\n        print(f\"\\nBalanced sample results:\\n{balanced_results}\")\n    except Exception as balanced_error:\n        print(f\"Error creating balanced evaluation sample: {balanced_error}\")\n    \n    # Save evaluation results\n    eval_output_dir = os.path.join(training_args.output_dir, \"eval_results\")\n    os.makedirs(eval_output_dir, exist_ok=True)\n    \n    with open(os.path.join(eval_output_dir, \"eval_results.json\"), \"w\") as f:\n        import json\n        results_dict = {\"validation\": val_results}\n        if 'test_results' in locals():\n            results_dict[\"test\"] = test_results\n        if 'balanced_results' in locals():\n            results_dict[\"balanced\"] = balanced_results\n        json.dump(results_dict, f, indent=4)\n    \n    print(f\"\\nEvaluation results saved to {eval_output_dir}/eval_results.json\")\nexcept Exception as e:\n    print(f\"\\nError during evaluation: {e}\")\n    raise\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Testing the Model with Example Predictions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Test model predictions on diverse examples\ntry:\n    print(\"\\nTesting model predictions on diverse examples...\")\n    \n    # Select diverse examples for testing\n    # 1. Get some examples from each class\n    val_labels = val_data['label']\n    class_0_indices = [i for i, label in enumerate(val_labels) if label == 0]\n    class_1_indices = [i for i, label in enumerate(val_labels) if label == 1]\n    \n    # 2. Sample from each class\n    import random\n    random.seed(42)  # For reproducibility\n    num_examples = min(5, min(len(class_0_indices), len(class_1_indices)))\n    \n    selected_indices = random.sample(class_0_indices, num_examples) + random.sample(class_1_indices, num_examples)\n    test_examples = val_data.select(selected_indices)\n    \n    # Tokenize examples\n    tokenized_test_examples = tokenizer(\n        test_examples[\"content\"],\n        padding=True,\n        truncation=True,\n        max_length=max_length,\n        return_tensors=\"pt\"\n    )\n    \n    # Move to device\n    for key, val in tokenized_test_examples.items():\n        if isinstance(val, torch.Tensor):\n            tokenized_test_examples[key] = val.to(device)\n    \n    # Get predictions\n    with torch.no_grad():\n        try:\n            # Use TPU-specific execution if available\n            if tpu_available:\n                import torch_xla.core.xla_model as xm\n                outputs = model(**{k: v for k, v in tokenized_test_examples.items() if k != \"label\"})\n                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n                predicted_labels = torch.argmax(predictions, dim=-1)\n                \n                # Move results to CPU\n                predictions = xm.mesh_reduce('predictions', predictions, lambda x: x)\n                predicted_labels = xm.mesh_reduce('predicted_labels', predicted_labels, lambda x: x)\n                \n                # Convert to numpy\n                predictions = predictions.cpu().numpy()\n                predicted_labels = predicted_labels.cpu().numpy()\n            else:\n                # Standard execution\n                outputs = model(**{k: v for k, v in tokenized_test_examples.items() if k != \"label\"})\n                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n                predicted_labels = torch.argmax(predictions, dim=-1).cpu().numpy()\n                predictions = predictions.cpu().numpy()\n        except Exception as pred_error:\n            print(f\"Error during prediction computation: {pred_error}\")\n            # Fallback to CPU if device-specific execution fails\n            model_cpu = model.cpu()\n            tokenized_cpu = {k: v.cpu() if isinstance(v, torch.Tensor) else v \n                           for k, v in tokenized_test_examples.items()}\n            outputs = model_cpu(**{k: v for k, v in tokenized_cpu.items() if k != \"label\"})\n            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1).numpy()\n            predicted_labels = np.argmax(predictions, axis=-1)\n    \n    # Print results with detailed information\n    print(\"\\nPrediction Results:\")\n    print(\"-\" * 80)\n    \n    for i, (pred, true) in enumerate(zip(predicted_labels, test_examples[\"label\"])):\n        is_package_swift = \"Yes\" if pred == 1 else \"No\"\n        true_is_package_swift = \"Yes\" if true == 1 else \"No\"\n        confidence = predictions[i][pred]\n        \n        # Determine if prediction is correct\n        correct = pred == true\n        result_marker = \"âœ“\" if correct else \"âœ—\"\n        \n        # Format output\n        print(f\"{result_marker} Example {i+1}:\")\n        print(f\"File path: {test_examples['path'][i]}\")\n        print(f\"Prediction: Is Package.swift? {is_package_swift} (Confidence: {confidence:.4f})\")\n        print(f\"True label: Is Package.swift? {true_is_package_swift}\")\n        \n        # Show content snippet with better formatting\n        content = test_examples['content'][i]\n        snippet = content[:200] + \"...\" if len(content) > 200 else content\n        print(f\"Content snippet:\\n{snippet}\")\n        print(\"-\" * 80)\n    \n    # Calculate accuracy on this sample\n    accuracy = sum(pred == true for pred, true in zip(predicted_labels, test_examples[\"label\"])) / len(predicted_labels)\n    print(f\"\\nAccuracy on these examples: {accuracy:.2f}\")\n    \n    # Save predictions to file\n    predictions_dir = os.path.join(training_args.output_dir, \"predictions\")\n    os.makedirs(predictions_dir, exist_ok=True)\n    \n    with open(os.path.join(predictions_dir, \"example_predictions.txt\"), \"w\") as f:\n        for i, (pred, true) in enumerate(zip(predicted_labels, test_examples[\"label\"])):\n            f.write(f\"Example {i+1}:\\n\")\n            f.write(f\"Path: {test_examples['path'][i]}\\n\")\n            f.write(f\"Predicted: {pred} (Confidence: {predictions[i][pred]:.4f})\\n\")\n            f.write(f\"True label: {true}\\n\")\n            f.write(f\"Content: {test_examples['content'][i][:100]}...\\n\\n\")\n    \n    print(f\"Predictions saved to {predictions_dir}/example_predictions.txt\")\nexcept Exception as e:\n    print(f\"\\nError during prediction testing: {e}\")\n    import traceback\n    traceback.print_exc()\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Saving the Model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Save model with versioning to prevent overwriting\ntry:\n    # Create timestamped directory to prevent overwriting previous models\n    from datetime import datetime\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    model_save_dir = f\"./codebert-swift-model_{timestamp}\"\n    os.makedirs(model_save_dir, exist_ok=True)\n    \n    print(f\"\\nSaving model to {model_save_dir}...\")\n    \n    # Move model to CPU for saving if on TPU\n    if tpu_available:\n        print(\"Moving model to CPU for saving...\")\n        cpu_model = model.cpu()\n        cpu_model.save_pretrained(model_save_dir)\n    else:\n        model.save_pretrained(model_save_dir)\n    \n    # Save tokenizer and configuration\n    tokenizer.save_pretrained(model_save_dir)\n    print(\"Tokenizer saved\")\n    \n    # Save model metadata\n    metadata = {\n        \"model_name\": \"CodeBERT for Swift\",\n        \"base_model\": MODEL_ID,\n        \"task\": \"Package.swift detection\",\n        \"training_dataset\": DATASET_ID,\n        \"training_date\": timestamp,\n        \"num_labels\": 2,\n        \"label_mapping\": {\"0\": \"Not Package.swift\", \"1\": \"Package.swift\"},\n        \"metrics\": val_results if 'val_results' in locals() else {},\n        \"hyperparameters\": {\n            \"learning_rate\": training_args.learning_rate,\n            \"batch_size\": training_args.per_device_train_batch_size,\n            \"epochs\": training_args.num_train_epochs,\n            \"max_length\": max_length\n        }\n    }\n    \n    with open(os.path.join(model_save_dir, \"model_metadata.json\"), \"w\") as f:\n        import json\n        json.dump(metadata, f, indent=2)\n    \n    # Create a symbolic link to the latest model\n    latest_link = \"./codebert-swift-model_latest\"\n    try:\n        if os.path.exists(latest_link) or os.path.islink(latest_link):\n            os.remove(latest_link)\n        os.symlink(model_save_dir, latest_link, target_is_directory=True)\n        print(f\"Created symbolic link {latest_link} -> {model_save_dir}\")\n    except Exception as link_error:\n        print(f\"Could not create symbolic link: {link_error}\")\n    \n    # Verify saved model\n    saved_files = os.listdir(model_save_dir)\n    print(f\"\\nVerifying saved model files: {saved_files}\")\n    \n    required_files = [\"pytorch_model.bin\", \"config.json\", \"tokenizer.json\"]\n    missing_files = [f for f in required_files if f not in saved_files]\n    \n    if missing_files:\n        print(f\"WARNING: Some required files are missing: {missing_files}\")\n    else:\n        print(\"All required model files were saved successfully\")\n    \n    print(f\"\\nModel and tokenizer saved to {model_save_dir}\")\n    \n    # Set the model_save_dir for the next steps\n    model_save_dir = latest_link\nexcept Exception as e:\n    print(f\"\\nError saving model: {e}\")\n    import traceback\n    traceback.print_exc()\n    raise\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Uploading to Dropbox",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import zipfile\nimport dropbox\nimport shutil\nfrom dropbox.files import WriteMode\nfrom dropbox.exceptions import ApiError\n\ndef zip_directory(directory, output_path, compression=zipfile.ZIP_DEFLATED):\n    \"\"\"Create a zip file from a directory with error handling and verification.\"\"\"\n    try:\n        # Ensure the directory exists\n        if not os.path.exists(directory):\n            raise FileNotFoundError(f\"Directory not found: {directory}\")\n            \n        print(f\"Zipping directory {directory} to {output_path}...\")\n        \n        # Create a temporary directory for staging files\n        temp_dir = f\"{directory}_temp\"\n        os.makedirs(temp_dir, exist_ok=True)\n        \n        # Copy files to temp directory to handle symlinks\n        for root, _, files in os.walk(directory):\n            for file in files:\n                src_path = os.path.join(root, file)\n                # Get relative path from the source directory\n                rel_path = os.path.relpath(src_path, directory)\n                # Create destination path in temp directory\n                dst_path = os.path.join(temp_dir, rel_path)\n                # Create parent directories if they don't exist\n                os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n                # Copy the file\n                shutil.copy2(src_path, dst_path)\n        \n        # Create the zip file from the temp directory\n        with zipfile.ZipFile(output_path, 'w', compression) as zipf:\n            file_count = 0\n            total_size = 0\n            \n            for root, _, files in os.walk(temp_dir):\n                for file in files:\n                    file_path = os.path.join(root, file)\n                    # Get path relative to temp directory for the archive\n                    rel_path = os.path.relpath(file_path, temp_dir)\n                    \n                    # Add file to zip\n                    zipf.write(file_path, rel_path)\n                    file_count += 1\n                    total_size += os.path.getsize(file_path)\n            \n            print(f\"Added {file_count} files ({total_size/1024/1024:.2f} MB) to zip archive\")\n        \n        # Clean up temp directory\n        shutil.rmtree(temp_dir)\n        \n        # Verify the zip file\n        if not os.path.exists(output_path):\n            raise FileNotFoundError(f\"Zip file was not created: {output_path}\")\n            \n        # Check zip file integrity\n        with zipfile.ZipFile(output_path, 'r') as zipf:\n            # Test the integrity of the zip file\n            test_result = zipf.testzip()\n            if test_result is not None:\n                raise zipfile.BadZipFile(f\"Zip file integrity test failed at: {test_result}\")\n            \n            # Count files in zip\n            zip_file_count = len(zipf.namelist())\n            print(f\"Zip file created successfully with {zip_file_count} files\")\n            \n        zip_size_mb = os.path.getsize(output_path) / (1024 * 1024)\n        print(f\"Zip file size: {zip_size_mb:.2f} MB\")\n        return True\n    except Exception as e:\n        print(f\"Error zipping directory: {e}\")\n        import traceback\n        traceback.print_exc()\n        return False\n\ntry:\n    # Create a unique zip filename with timestamp\n    from datetime import datetime\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    model_zip_path = f\"./codebert-swift-model_{timestamp}.zip\"\n    \n    # Zip the model directory\n    if zip_directory(model_save_dir, model_zip_path):\n        print(f\"Successfully created zip file at {model_zip_path}\")\n        \n        # Create a symbolic link to the latest zip file\n        latest_zip_link = \"./codebert-swift-model_latest.zip\"\n        try:\n            if os.path.exists(latest_zip_link) or os.path.islink(latest_zip_link):\n                os.remove(latest_zip_link)\n            os.symlink(model_zip_path, latest_zip_link)\n            print(f\"Created symbolic link {latest_zip_link} -> {model_zip_path}\")\n        except Exception as link_error:\n            print(f\"Could not create symbolic link: {link_error}\")\n    else:\n        print(\"Failed to create zip file. Aborting upload process.\")\n        raise ValueError(\"Zip file creation failed\")\nexcept Exception as e:\n    print(f\"Error preparing zip: {e}\")\n    raise\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Dropbox authentication and client setup\nAPP_KEY = \"2bi422xpd3xd962\"\nAPP_SECRET = \"j3yx0b41qdvfu86\"\nREFRESH_TOKEN = \"RvyL03RE5qAAAAAAAAAAAVMVebvE7jDx8Okd0ploMzr85c6txvCRXpJAt30mxrKF\"\n\ndef get_dropbox_client(max_retries=3, retry_delay=5):\n    \"\"\"Get a Dropbox client with retry logic.\"\"\"\n    from dropbox.exceptions import AuthError\n    \n    for attempt in range(max_retries):\n        try:\n            print(f\"Connecting to Dropbox (attempt {attempt+1}/{max_retries})...\")\n            dbx = dropbox.Dropbox(\n                app_key=APP_KEY, \n                app_secret=APP_SECRET, \n                oauth2_refresh_token=REFRESH_TOKEN,\n                timeout=60  # Increased timeout for reliability\n            )\n            \n            # Verify connection by getting account info\n            account = dbx.users_get_current_account()\n            print(f\"Connected to Dropbox as {account.name.display_name} ({account.email})\")\n            return dbx\n        except AuthError as e:\n            print(f\"ERROR: Authentication failed (attempt {attempt+1}). {e}\")\n            if attempt < max_retries - 1:\n                print(f\"Retrying in {retry_delay} seconds...\")\n                time.sleep(retry_delay)\n            else:\n                print(\"All authentication attempts failed.\")\n                return None\n        except Exception as e:\n            print(f\"ERROR: Dropbox connection error (attempt {attempt+1}). {e}\")\n            if attempt < max_retries - 1:\n                print(f\"Retrying in {retry_delay} seconds...\")\n                time.sleep(retry_delay)\n            else:\n                print(\"All connection attempts failed.\")\n                return None\n    \n    return None\n\n# Test Dropbox connection\ntry:\n    dbx_client = get_dropbox_client()\n    if dbx_client:\n        print(\"Dropbox client initialized successfully\")\n    else:\n        print(\"WARNING: Could not initialize Dropbox client. Upload functionality will be disabled.\")\nexcept Exception as e:\n    print(f\"Error initializing Dropbox client: {e}\")\n    dbx_client = None\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def upload_file_to_dropbox(file_path, dropbox_path, max_retries=3, retry_delay=10):\n    \"\"\"Upload a file to Dropbox with robust error handling and retry logic.\"\"\"\n    # Verify the file exists and is readable\n    if not os.path.exists(file_path):\n        print(f\"ERROR: File not found: {file_path}\")\n        return False\n        \n    if not os.access(file_path, os.R_OK):\n        print(f\"ERROR: File not readable: {file_path}\")\n        return False\n    \n    # Get file size for progress tracking\n    file_size = os.path.getsize(file_path)\n    if file_size == 0:\n        print(f\"ERROR: File is empty: {file_path}\")\n        return False\n        \n    print(f\"Preparing to upload {file_path} ({file_size/1024/1024:.2f} MB) to Dropbox as {dropbox_path}\")\n    \n    # Get Dropbox client\n    if 'dbx_client' in globals() and dbx_client:\n        dbx = dbx_client\n    else:\n        dbx = get_dropbox_client()\n        \n    if not dbx:\n        print(\"ERROR: Could not initialize Dropbox client\")\n        return False\n    \n    # Chunk size for large file uploads (4MB)\n    chunk_size = 4 * 1024 * 1024\n    \n    # Retry loop for the entire upload process\n    for upload_attempt in range(max_retries):\n        try:\n            print(f\"\\nUpload attempt {upload_attempt+1}/{max_retries}\")\n            \n            # Small file upload (single request)\n            if file_size <= chunk_size:\n                print(f\"Uploading file in a single request...\")\n                with open(file_path, 'rb') as f:\n                    result = dbx.files_upload(\n                        f.read(), \n                        dropbox_path, \n                        mode=WriteMode('overwrite'),\n                        mute=False\n                    )\n                print(f\"Upload complete! File ID: {result.id}\")\n                return True\n            \n            # Large file upload (chunked)\n            print(f\"Uploading file in chunks...\")\n            \n            # Open the file and start the upload session\n            with open(file_path, 'rb') as f:\n                # Start upload session and get session ID\n                print(\"Starting upload session...\")\n                upload_session_start_result = dbx.files_upload_session_start(f.read(chunk_size))\n                session_id = upload_session_start_result.session_id\n                \n                # Initialize cursor and progress tracking\n                uploaded = f.tell()\n                cursor = dropbox.files.UploadSessionCursor(session_id=session_id, offset=uploaded)\n                commit = dropbox.files.CommitInfo(path=dropbox_path, mode=WriteMode('overwrite'))\n                \n                # Progress bar for tracking upload\n                with tqdm(total=file_size, desc=\"Uploading\", unit=\"B\", unit_scale=True) as pbar:\n                    pbar.update(uploaded)\n                    \n                    # Upload the file in chunks\n                    while uploaded < file_size:\n                        # Check if this is the last chunk\n                        if (file_size - uploaded) <= chunk_size:\n                            print(\"\\nUploading final chunk...\")\n                            data = f.read(chunk_size)\n                            result = dbx.files_upload_session_finish(data, cursor, commit)\n                            uploaded += len(data)\n                            pbar.update(len(data))\n                            print(f\"Upload complete! File ID: {result.id}\")\n                        else:\n                            # Upload intermediate chunk\n                            data = f.read(chunk_size)\n                            dbx.files_upload_session_append_v2(data, cursor)\n                            uploaded += len(data)\n                            cursor.offset = uploaded\n                            pbar.update(len(data))\n                            \n                            # Periodically report progress\n                            if uploaded % (10 * chunk_size) == 0:\n                                progress_pct = (uploaded / file_size) * 100\n                                print(f\"\\nProgress: {progress_pct:.1f}% ({uploaded/1024/1024:.2f} MB / {file_size/1024/1024:.2f} MB)\")\n                \n                print(\"\\nChunked upload completed successfully!\")\n                return True\n                \n        except ApiError as e:\n            print(f\"\\nERROR: Dropbox API error during upload (attempt {upload_attempt+1}): {e}\")\n            # Check for specific error types that might be recoverable\n            if hasattr(e, 'error') and isinstance(e.error, dropbox.exceptions.InternalServerError):\n                print(\"This appears to be a temporary Dropbox server issue. Retrying...\")\n            elif hasattr(e, 'error') and isinstance(e.error, dropbox.exceptions.RateLimitError):\n                # For rate limit errors, wait longer\n                retry_delay = max(retry_delay * 2, 60)  # Exponential backoff, max 60 seconds\n                print(f\"Rate limit exceeded. Waiting {retry_delay} seconds before retry...\")\n            \n            if upload_attempt < max_retries - 1:\n                print(f\"Retrying in {retry_delay} seconds...\")\n                time.sleep(retry_delay)\n            else:\n                print(\"All upload attempts failed.\")\n                return False\n                \n        except Exception as e:\n            print(f\"\\nERROR: Unexpected error during upload (attempt {upload_attempt+1}): {e}\")\n            import traceback\n            traceback.print_exc()\n            \n            if upload_attempt < max_retries - 1:\n                print(f\"Retrying in {retry_delay} seconds...\")\n                time.sleep(retry_delay)\n            else:\n                print(\"All upload attempts failed.\")\n                return False\n    \n    return False\n\n# Upload the model to Dropbox\ntry:\n    # Create a unique Dropbox path with timestamp\n    from datetime import datetime\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    dropbox_folder = \"/codebert-swift-model\"\n    dropbox_path = f\"{dropbox_folder}/codebert-swift-model_{timestamp}.zip\"\n    \n    print(f\"\\nUploading model to Dropbox at {dropbox_path}...\")\n    \n    # Create the target folder if it doesn't exist\n    if dbx_client:\n        try:\n            dbx_client.files_create_folder_v2(dropbox_folder)\n            print(f\"Created Dropbox folder: {dropbox_folder}\")\n        except ApiError as e:\n            # Ignore error if folder already exists\n            if not isinstance(e.error, dropbox.files.CreateFolderError) or \\\n               not e.error.is_path() or \\\n               not e.error.get_path().is_conflict():\n                print(f\"Warning: {e}\")\n    \n    # Set up max retries and retry delay\n    max_upload_retries = 5\n    retry_delay = 15\n    \n    # Attempt to upload with retries\n    if dbx_client:\n        upload_success = upload_file_to_dropbox(\n            file_path=model_zip_path, \n            dropbox_path=dropbox_path,\n            max_retries=max_upload_retries,\n            retry_delay=retry_delay\n        )\n        \n        if upload_success:\n            print(f\"\\nSuccessfully uploaded model to Dropbox at {dropbox_path}\")\n        else:\n            print(f\"\\nFailed to upload model to Dropbox after {max_upload_retries} attempts.\")\n    else:\n        print(\"\\nSkipping upload: Dropbox client not available\")\n        \nexcept Exception as e:\n    print(f\"\\nError in upload process: {e}\")\n    import traceback\n    traceback.print_exc()\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Creating a Shareable Link",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def create_shared_link(dropbox_path, max_retries=3, retry_delay=5):\n    \"\"\"Create a shared link for a Dropbox file with retry logic.\"\"\"\n    # Get Dropbox client\n    if 'dbx_client' in globals() and dbx_client:\n        dbx = dbx_client\n    else:\n        dbx = get_dropbox_client()\n        \n    if not dbx:\n        print(\"ERROR: Could not initialize Dropbox client\")\n        return None\n    \n    # Retry loop for creating shared link\n    for attempt in range(max_retries):\n        try:\n            print(f\"Creating shared link (attempt {attempt+1}/{max_retries})...\")\n            \n            # Create a shared link with download permissions\n            shared_link_settings = dropbox.sharing.SharedLinkSettings(\n                requested_visibility=dropbox.sharing.RequestedVisibility.public,\n                audience=dropbox.sharing.LinkAudience.public,\n                access=dropbox.sharing.AccessLevel.viewer,\n                allow_download=True\n            )\n            \n            shared_link_metadata = dbx.sharing_create_shared_link_with_settings(\n                dropbox_path, \n                settings=shared_link_settings\n            )\n            \n            print(f\"Shared link created: {shared_link_metadata.url}\")\n            return shared_link_metadata.url\n            \n        except ApiError as e:\n            # Handle case where link already exists\n            if isinstance(e.error, dropbox.sharing.CreateSharedLinkWithSettingsError) and \\\n               e.error.is_shared_link_already_exists():\n                print(\"A shared link already exists for this file. Retrieving existing link...\")\n                try:\n                    links = dbx.sharing_list_shared_links(dropbox_path).links\n                    if links:\n                        print(f\"Retrieved existing shared link: {links[0].url}\")\n                        return links[0].url\n                    else:\n                        print(\"No existing links found despite 'already exists' error.\")\n                except Exception as list_error:\n                    print(f\"Error retrieving existing links: {list_error}\")\n            \n            print(f\"ERROR: Dropbox API error (attempt {attempt+1}): {e}\")\n            \n            if attempt < max_retries - 1:\n                print(f\"Retrying in {retry_delay} seconds...\")\n                time.sleep(retry_delay)\n            else:\n                print(\"All attempts to create shared link failed.\")\n                return None\n                \n        except Exception as e:\n            print(f\"ERROR: Unexpected error (attempt {attempt+1}): {e}\")\n            \n            if attempt < max_retries - 1:\n                print(f\"Retrying in {retry_delay} seconds...\")\n                time.sleep(retry_delay)\n            else:\n                print(\"All attempts to create shared link failed.\")\n                return None\n    \n    return None\n\n# Create a shareable download link\ntry:\n    if 'upload_success' in locals() and upload_success and dbx_client:\n        print(\"\\nCreating shareable download link...\")\n        \n        # Create shared link with retry logic\n        shared_link = create_shared_link(dropbox_path)\n        \n        if shared_link:\n            # Convert to direct download link\n            download_link = shared_link.replace(\"www.dropbox.com\", \"dl.dropboxusercontent.com\").replace(\"?dl=0\", \"\")\n            print(f\"\\nDownload link: {download_link}\")\n            \n            # Save link to a file for reference\n            links_file = \"./model_download_links.txt\"\n            with open(links_file, \"a\") as f:\n                f.write(f\"\\n--- {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ---\\n\")\n                f.write(f\"Model: codebert-swift-model_{timestamp}.zip\\n\")\n                f.write(f\"Dropbox path: {dropbox_path}\\n\")\n                f.write(f\"Shared link: {shared_link}\\n\")\n                f.write(f\"Download link: {download_link}\\n\\n\")\n            \n            print(f\"Links saved to {links_file}\")\n        else:\n            print(\"\\nFailed to create shared link.\")\n    elif not dbx_client:\n        print(\"\\nSkipping shared link creation: Dropbox client not available\")\n    else:\n        print(\"\\nSkipping shared link creation: Upload was not successful\")\nexcept Exception as e:\n    print(f\"\\nError in link creation: {e}\")\n    import traceback\n    traceback.print_exc()\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Conclusion\n\n1. Loaded and prepared the Swift code dataset\n2. Fine-tuned CodeBERT for `Package.swift` classification\n3. Evaluated and tested the model\n4. Saved and uploaded to Dropbox\n\nThis model can be used for Swift code understanding tasks.",
   "metadata": {}
  }
 ]
}